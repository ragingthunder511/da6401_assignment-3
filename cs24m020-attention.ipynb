{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11850506,"sourceType":"datasetVersion","datasetId":7446208}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Core Libraries ---\nimport os                # Operating system interactions\nimport random            # Random number generation\nimport re                # Regular expressions for text processing\n\n# --- Data Handling and Analysis ---\nimport pandas as pd      # Data manipulation and analysis\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt  # Plotting library\nfrom matplotlib import rcParams   # Global plotting configuration\nimport seaborn as sns     # Statistical data visualization\nfrom tqdm import tqdm     # Progress bars for iterables\n\n# --- PyTorch ---\nimport torch              # Main PyTorch library\nimport torch.nn as nn     # Neural network modules\nimport torch.nn.functional as F  # Functional interface for NN operations\nimport torch.optim as optim  # Optimization algorithms\nfrom torch.utils.data import Dataset, DataLoader  # Data loading utilities\n\n# --- Experiment Tracking ---\nimport wandb             # Weights & Biases for experiment logging\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:23.876326Z","iopub.execute_input":"2025-05-21T18:30:23.876527Z","iopub.status.idle":"2025-05-21T18:30:31.685264Z","shell.execute_reply.started":"2025-05-21T18:30:23.876509Z","shell.execute_reply":"2025-05-21T18:30:31.684668Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# === Configuration and Setup ===\n\n# Base directory for Dakshina dataset lexicons\n# Modify the language code ('hi' for Hindi) as needed\ndataset_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons'\n\n# Select computation device: GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Set the Weights & Biases API key for logging experiments\nWANDB_KEY = \"8f2f82255a6e5ea16321da3895ae6b00d50eb5b5\"\nos.environ.setdefault(\"WANDB_API_KEY\", WANDB_KEY)\ntry:\n    # ‘relogin=True’ forces a fresh session if needed\n    wandb.login(key=WANDB_KEY, relogin=True)\nexcept wandb.errors.UsageError:\n    # Already authenticated or bad key—ignore quietly\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:31.686890Z","iopub.execute_input":"2025-05-21T18:30:31.687247Z","iopub.status.idle":"2025-05-21T18:30:38.197812Z","shell.execute_reply.started":"2025-05-21T18:30:31.687231Z","shell.execute_reply":"2025-05-21T18:30:38.197218Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m020\u001b[0m (\u001b[33mcs24m020-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CharEmbedder(nn.Module):\n    \"\"\"\n    Converts sequences of character indices into dense embeddings.\n\n    Args:\n        vocab_size (int): Number of unique characters in the vocabulary.\n        embed_dim (int): Dimensionality of each character embedding.\n    \"\"\"\n    def __init__(self, vocab_size: int, embed_dim: int):\n        super().__init__()\n        # Embedding layer: maps each input index to an embedding vector\n        self.char_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n\n    def forward(self, char_seq: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Produce embeddings for an input batch of character sequences.\n\n        Args:\n            char_seq (Tensor): Shape (batch_size, seq_len) with character indices.\n\n        Returns:\n            Tensor: Shape (batch_size, seq_len, embed_dim) of embeddings.\n        \"\"\"\n        return self.char_embedding(char_seq)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.198522Z","iopub.execute_input":"2025-05-21T18:30:38.198986Z","iopub.status.idle":"2025-05-21T18:30:38.203786Z","shell.execute_reply.started":"2025-05-21T18:30:38.198966Z","shell.execute_reply":"2025-05-21T18:30:38.203079Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class SequenceEncoder(nn.Module):\n    \"\"\"\n    Encodes token sequences into contextual representations using RNN variants.\n    Supports GRU, LSTM, or vanilla RNN with optional bidirectionality and dropout.\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        embed_dim: int,\n        hidden_dim: int,\n        num_layers: int = 1,\n        cell: str = 'GRU',\n        dropout: float = 0.1,\n        bidirectional: bool = False\n    ) -> None:\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        rnn_dropout = dropout if num_layers > 1 else 0.0\n\n        # Token embedding + pre-RNN dropout\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.drop  = nn.Dropout(dropout)\n\n        # Choose RNN cell\n        cell_map = {\n            'GRU': nn.GRU,\n            'LSTM': nn.LSTM,\n            'RNN': lambda *args, **kwargs: nn.RNN(*args, nonlinearity='tanh', **kwargs)\n        }\n        rnn_cls = cell_map.get(cell, nn.GRU)\n        self.rnn = rnn_cls(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=rnn_dropout,\n            bidirectional=bidirectional,\n            batch_first=True\n        )\n\n    def forward(self, tokens: torch.Tensor) -> tuple:\n        \"\"\"\n        Args:\n            tokens (Tensor): (batch, seq_len) token indices\n        Returns:\n            outputs: (batch, seq_len, hidden_dim * directions)\n            hidden: final hidden state tuple or tensor\n        \"\"\"\n        emb = self.drop(self.embed(tokens))\n        outputs, hidden = self.rnn(emb)\n        return outputs, hidden\n\n\nclass SoftDotAttention(nn.Module):\n    \"\"\"\n    Computes soft attention weights and context vectors using dot-product style.\n    \"\"\"\n    def __init__(self, hidden_dim: int) -> None:\n        super().__init__()\n        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.v = nn.Parameter(torch.Tensor(hidden_dim))\n        nn.init.uniform_(self.v, -0.1, 0.1)\n\n    def forward(self, h_t: torch.Tensor, encoder_outputs: torch.Tensor) -> torch.Tensor:\n        # h_t: (batch, hidden), encoder_outputs: (batch, seq_len, hidden)\n        batch, seq_len, _ = encoder_outputs.size()\n        # Expand h_t for concatenation\n        h_exp = h_t.unsqueeze(1).expand(-1, seq_len, -1)\n        # Compute energy scores\n        energy = torch.tanh(self.linear(torch.cat([h_exp, encoder_outputs], dim=2)))\n        energy = energy.transpose(1, 2)  # (batch, hidden, seq_len)\n        v_exp = self.v.unsqueeze(0).expand(batch, -1).unsqueeze(1)  # (batch,1,hidden)\n        scores = torch.bmm(v_exp, energy).squeeze(1)  # (batch, seq_len)\n        return F.softmax(scores, dim=1)  # attention weights\n\n\nclass AttentiveDecoder(nn.Module):\n    \"\"\"\n    Decoder with attention: generates output tokens step-by-step.\n    \"\"\"\n    def __init__(\n        self,\n        out_vocab: int,\n        embed_dim: int,\n        hidden_dim: int,\n        num_layers: int = 1,\n        cell: str = 'GRU',\n        dropout: float = 0.1\n    ) -> None:\n        super().__init__()\n        rnn_dropout = dropout if num_layers > 1 else 0.0\n\n        self.embed = nn.Embedding(out_vocab, embed_dim)\n        self.drop  = nn.Dropout(dropout)\n\n        # RNN input will include context from attention\n        rnn_input_dim = embed_dim + hidden_dim\n        cell_map = {'GRU': nn.GRU, 'LSTM': nn.LSTM, 'RNN': nn.RNN}\n        rnn_cls = cell_map.get(cell, nn.GRU)\n        self.rnn = rnn_cls(\n            input_size=rnn_input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=rnn_dropout,\n            batch_first=True\n        )\n\n        self.attn   = SoftDotAttention(hidden_dim)\n        self.out_dp = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_dim, out_vocab)\n\n    def forward(\n        self,\n        input_step: torch.Tensor,\n        prev_hidden,\n        encoder_outputs: torch.Tensor\n    ) -> tuple:\n        # input_step: (batch, 1), encoder_outputs: (batch, seq_len, hidden)\n        emb = self.drop(self.embed(input_step.squeeze(1))).unsqueeze(1)\n        # Get last hidden state\n        if isinstance(prev_hidden, tuple):  # LSTM\n            h_last = prev_hidden[0][-1]\n        else:\n            h_last = prev_hidden[-1]\n        # Compute attention and context\n        attn_w = self.attn(h_last, encoder_outputs).unsqueeze(1)  # (batch,1,seq_len)\n        context = torch.bmm(attn_w, encoder_outputs)  # (batch,1,hidden)\n        # RNN input\n        rnn_in = torch.cat([emb, context], dim=2)\n        output, hidden = self.rnn(rnn_in, prev_hidden)\n        # Generate final output distribution\n        logits = self.fc_out(self.out_dp(output.squeeze(1)))  # (batch, out_vocab)\n        return F.log_softmax(logits, dim=1), hidden, attn_w.squeeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.204634Z","iopub.execute_input":"2025-05-21T18:30:38.204856Z","iopub.status.idle":"2025-05-21T18:30:38.225771Z","shell.execute_reply.started":"2025-05-21T18:30:38.204831Z","shell.execute_reply":"2025-05-21T18:30:38.225194Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def beam_search(\n    model,\n    src_seq: torch.Tensor,\n    sos_idx: int,\n    eos_idx: int,\n    max_len: int = 30,\n    beam_width: int = 3,\n    device: str = 'cuda'\n) -> list:\n    \"\"\"\n    Performs beam search decoding on a seq2seq model with attention.\n\n    Args:\n        model: Seq2Seq model with encoder and decoder attributes.\n        src_seq (Tensor): Input sequence tensor (1, seq_len).\n        sos_idx (int): Start-of-sequence token index.\n        eos_idx (int): End-of-sequence token index.\n        max_len (int): Maximum decoding steps.\n        beam_width (int): Number of beams to maintain.\n        device (str): Device to perform computations.\n\n    Returns:\n        List of tuples: (token sequence list, cumulative log-probability).\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Encode input\n        enc_outs, enc_hidden = model.encoder(src_seq.to(device))\n\n        # Initialize decoder hidden state\n        if model.bidirectional:\n            # Combine bidirectional states\n            if model.cell == 'LSTM':\n                h_n, c_n = enc_hidden\n                layers = model.encoder.num_layers\n                h_dec = torch.zeros(layers, 1, model.decoder.hidden_dim, device=device)\n                c_dec = torch.zeros(layers, 1, model.decoder.hidden_dim, device=device)\n                for i in range(layers):\n                    h_cat = torch.cat([h_n[2*i], h_n[2*i+1]], dim=1)\n                    c_cat = torch.cat([c_n[2*i], c_n[2*i+1]], dim=1)\n                    h_dec[i] = model.hidden_transform(h_cat)\n                    c_dec[i] = model.hidden_transform(c_cat)\n                dec_hidden = (h_dec, c_dec)\n            else:\n                layers = model.encoder.num_layers\n                h_dec = torch.zeros(layers, 1, model.decoder.hidden_dim, device=device)\n                for i in range(layers):\n                    h_cat = torch.cat([enc_hidden[2*i], enc_hidden[2*i+1]], dim=1)\n                    h_dec[i] = model.hidden_transform(h_cat)\n                dec_hidden = h_dec\n        else:\n            dec_hidden = enc_hidden\n\n        # Beam candidates: (sequence, score, hidden)\n        beams = [([sos_idx], 0.0, dec_hidden)]\n        completed = []\n\n        for _ in range(max_len):\n            candidates = []\n            for seq, score, hidden in beams:\n                # If EOS reached, collect result\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n                inp = torch.tensor([[seq[-1]]], device=device)\n                # Decode step\n                output, hidden_n, _ = model.decoder(inp, hidden, enc_outs)\n                topk = torch.topk(output.squeeze(0), beam_width)\n                for logp, idx in zip(topk.values, topk.indices):\n                    new_seq = seq + [idx.item()]\n                    new_score = score + logp.item()\n                    # Detach hidden to prevent graph growth\n                    if isinstance(hidden_n, tuple):\n                        hn = tuple(h.detach() for h in hidden_n)\n                    else:\n                        hn = hidden_n.detach()\n                    candidates.append((new_seq, new_score, hn))\n            # Select top beams\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        # Include any beams ended with EOS\n        completed += [(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx]\n        # If none completed, use current beams\n        if not completed:\n            completed = beams\n        # Sort by highest score\n        return sorted(completed, key=lambda x: x[1], reverse=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.226450Z","iopub.execute_input":"2025-05-21T18:30:38.226691Z","iopub.status.idle":"2025-05-21T18:30:38.245502Z","shell.execute_reply.started":"2025-05-21T18:30:38.226676Z","shell.execute_reply":"2025-05-21T18:30:38.244897Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    \"\"\"\n    End-to-end seq2seq model with optional bidirectional encoder and attention decoder.\n    \"\"\"\n    def __init__(\n        self,\n        src_vocab: int,\n        tgt_vocab: int,\n        embed_dim: int = 256,\n        hidden_dim: int = 256,\n        enc_layers: int = 1,\n        dec_layers: int = 1,\n        cell_type: str = 'GRU',\n        dropout: float = 0.2,\n        bidirectional: bool = False\n    ) -> None:\n        super().__init__()\n        self.encoder = SequenceEncoder(src_vocab, embed_dim, hidden_dim, enc_layers, cell_type, dropout, bidirectional)\n        self.decoder = AttentiveDecoder(tgt_vocab, embed_dim, hidden_dim, dec_layers, cell_type, dropout)\n        self.bidirectional = bidirectional\n        if bidirectional:\n            self.hidden_transform = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.cell_type = cell_type\n\n    def _match_decoder(self, hidden, batch_size: int):\n        \"\"\"Match encoder hidden dims to decoder layers.\"\"\"\n        if isinstance(hidden, tuple):  # LSTM\n            h, c = hidden\n            return (self._pad_or_trim(h, batch_size), self._pad_or_trim(c, batch_size))\n        return self._pad_or_trim(hidden, batch_size)\n\n    def _pad_or_trim(self, h: torch.Tensor, batch_size: int) -> torch.Tensor:\n        layers = self.decoder.rnn.num_layers\n        if h.size(0) > layers:\n            return h[:layers]\n        if h.size(0) < layers:\n            pad = h.new_zeros(layers - h.size(0), batch_size, h.size(2))\n            return torch.cat([h, pad], dim=0)\n        return h\n\n    def _combine_bidirectional(self, hidden):\n        \"\"\"Transform bidirectional encoder states for decoder initialization.\"\"\"\n        if self.cell_type == 'LSTM':\n            h, c = hidden\n            return (self._merge_dirs(h), self._merge_dirs(c))\n        return self._merge_dirs(hidden)\n\n    def _merge_dirs(self, h: torch.Tensor) -> torch.Tensor:\n        # h: (2*layers, batch, hidden)\n        layers = h.size(0) // 2\n        merged = []\n        for i in range(layers):\n            cat = torch.cat([h[2*i], h[2*i+1]], dim=1)\n            merged.append(self.hidden_transform(cat))\n        return torch.stack(merged)\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        tgt: torch.Tensor,\n        teacher_forcing: float = 0.5,\n        return_attn: bool = False\n    ):\n        batch, tgt_len = tgt.size()\n        outputs = src.new_zeros(batch, tgt_len, self.decoder.fc_out.out_features, dtype=torch.float)\n        attn_weights = [] if return_attn else None\n\n        enc_outs, enc_hidden = self.encoder(src)\n        dec_hidden = self._combine_bidirectional(enc_hidden) if self.bidirectional else self._match_decoder(enc_hidden, batch)\n\n        input_tok = tgt[:, 0].unsqueeze(1)\n        for t in range(1, tgt_len):\n            out, dec_hidden, attn = self.decoder(input_tok, dec_hidden, enc_outs)\n            outputs[:, t] = out\n            if return_attn:\n                attn_weights.append(attn.unsqueeze(1))\n            if random.random() < teacher_forcing:\n                input_tok = tgt[:, t].unsqueeze(1)\n            else:\n                input_tok = out.argmax(1).unsqueeze(1)\n\n        if return_attn:\n            return outputs, torch.cat(attn_weights, dim=1)\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.246358Z","iopub.execute_input":"2025-05-21T18:30:38.246600Z","iopub.status.idle":"2025-05-21T18:30:38.265604Z","shell.execute_reply.started":"2025-05-21T18:30:38.246577Z","shell.execute_reply":"2025-05-21T18:30:38.264843Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class LexiconDataset(Dataset):\n    \"\"\"\n    Dataset for transliteration pairs loaded from a TSV file.\n    Each line should contain target and source strings separated by a tab.\n\n    Args:\n        file_path (str): Path to the TSV file.\n        src_vocab (dict, optional): Pre-built source vocabulary mapping.\n        tgt_vocab (dict, optional): Pre-built target vocabulary mapping.\n        build_vocab (bool): Whether to construct vocabularies from data.\n    \"\"\"\n    def __init__(\n        self,\n        file_path: str,\n        src_vocab: dict = None,\n        tgt_vocab: dict = None,\n        build_vocab: bool = False\n    ) -> None:\n        super().__init__()\n        self.pairs = []\n        # Read TSV and collect (source, target) pairs\n        with open(file_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) < 2:\n                    continue\n                tgt_text, src_text = parts[0], parts[1]\n                self.pairs.append((src_text, tgt_text))\n\n        if build_vocab:\n            # Initialize special tokens\n            self.src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n            self.tgt_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n            # Populate with characters from data\n            for src_text, tgt_text in self.pairs:\n                for ch in src_text:\n                    self.src_vocab.setdefault(ch, len(self.src_vocab))\n                for ch in tgt_text:\n                    self.tgt_vocab.setdefault(ch, len(self.tgt_vocab))\n        else:\n            assert src_vocab is not None and tgt_vocab is not None, \\\n                \"Provide src_vocab and tgt_vocab if not building them.\"\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def __getitem__(self, idx: int) -> tuple:\n        src_text, tgt_text = self.pairs[idx]\n        # Encode source chars\n        src_indices = [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src_text]\n        # Encode target with <sos> and <eos>\n        tgt_indices = [self.tgt_vocab['<sos>']] + \\\n                      [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt_text] + \\\n                      [self.tgt_vocab['<eos>']]\n        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)\n\n\ndef pad_collate(batch: list) -> tuple:\n    \"\"\"\n    Pads source and target sequences in a batch to the maximum lengths.\n\n    Args:\n        batch (list): List of (src_tensor, tgt_tensor) pairs.\n\n    Returns:\n        padded_src (Tensor): (batch_size, max_src_len)\n        padded_tgt (Tensor): (batch_size, max_tgt_len)\n    \"\"\"\n    srcs, tgts = zip(*batch)\n    max_src = max(s.size(0) for s in srcs)\n    max_tgt = max(t.size(0) for t in tgts)\n\n    padded_src = torch.full((len(batch), max_src), fill_value=0, dtype=torch.long)\n    padded_tgt = torch.full((len(batch), max_tgt), fill_value=0, dtype=torch.long)\n    for i, (s, t) in enumerate(zip(srcs, tgts)):\n        padded_src[i, :s.size(0)] = s\n        padded_tgt[i, :t.size(0)] = t\n    return padded_src, padded_tgt\n\n\ndef get_dataloaders(\n    base_dir: str,\n    batch_size: int,\n    build_vocab: bool = False\n) -> tuple:\n    \"\"\"\n    Creates DataLoaders for train, validation, and test splits.\n\n    Args:\n        base_dir (str): Directory containing 'hi.translit.sampled.*.tsv' files.\n        batch_size (int): Batch size for DataLoaders.\n        build_vocab (bool): Whether to build vocabulary from training data.\n\n    Returns:\n        train_loader, val_loader, test_loader,\n        src_vocab_size, tgt_vocab_size, pad_token_idx,\n        src_vocab, tgt_vocab\n    \"\"\"\n    # Paths for splits\n    train_fp = os.path.join(base_dir, 'hi.translit.sampled.train.tsv')\n    val_fp   = os.path.join(base_dir, 'hi.translit.sampled.dev.tsv')\n    test_fp  = os.path.join(base_dir, 'hi.translit.sampled.test.tsv')\n\n    # Instantiate datasets\n    train_ds = LexiconDataset(train_fp, build_vocab=build_vocab)\n    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n    val_ds = LexiconDataset(val_fp, src_vocab, tgt_vocab)\n    test_ds = LexiconDataset(test_fp, src_vocab, tgt_vocab)\n\n    # Dataloaders\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n    test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=pad_collate)\n\n    return (\n        train_loader, val_loader, test_loader,\n        len(src_vocab), len(tgt_vocab), src_vocab['<pad>'],\n        src_vocab, tgt_vocab\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.267458Z","iopub.execute_input":"2025-05-21T18:30:38.267844Z","iopub.status.idle":"2025-05-21T18:30:38.284285Z","shell.execute_reply.started":"2025-05-21T18:30:38.267822Z","shell.execute_reply":"2025-05-21T18:30:38.283659Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class EarlyStopper:\n    \"\"\"\n    Monitors a metric and stops training if it does not improve within patience epochs.\n    \n    Args:\n        patience (int): Number of epochs with no improvement before stopping.\n        min_delta (float): Minimum change to qualify as improvement.\n    \"\"\"\n    def __init__(self, patience: int = 5, min_delta: float = 1e-4) -> None:\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_score = None\n\n    def should_stop(self, current_score: float) -> bool:\n        \"\"\"\n        Returns True if training should be stopped based on the monitored metric.\n        \"\"\"\n        if self.best_score is None or current_score > self.best_score + self.min_delta:\n            self.best_score = current_score\n            self.counter = 0\n        else:\n            self.counter += 1\n        return self.counter >= self.patience","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.284909Z","iopub.execute_input":"2025-05-21T18:30:38.285137Z","iopub.status.idle":"2025-05-21T18:30:38.302492Z","shell.execute_reply.started":"2025-05-21T18:30:38.285121Z","shell.execute_reply":"2025-05-21T18:30:38.301949Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'early_terminate': {\n        'type': 'hyperband',\n        'min_iter': 2,\n        'max_iter': 8,\n        's': 2\n    },\n    'parameters': {\n        'embedding_dim': {'values': [16, 32, 64, 256]},\n        'hidden_size':    {'values': [16, 32, 64, 256]},\n        'encoder_layers': {'values': [1, 2, 3]},\n        'decoder_layers': {'values': [1, 2, 3]},\n        'cell_type':      {'values': ['RNN', 'GRU', 'LSTM']},\n        'dropout_p':      {'values': [0.2, 0.3, 0.4]},\n        'beam_width':     {'values': [1, 3, 5]},\n        'teacher_forcing_ratio': {'values': [0.0, 0.3, 0.5, 0.7, 1.0]}\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:38.303345Z","iopub.execute_input":"2025-05-21T18:30:38.303566Z","iopub.status.idle":"2025-05-21T18:30:38.321267Z","shell.execute_reply.started":"2025-05-21T18:30:38.303551Z","shell.execute_reply":"2025-05-21T18:30:38.320503Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_run():\n    \"\"\"\n    Executes one run of training, validation, and testing within a W&B sweep context.\n    \"\"\"\n    with wandb.init():\n        cfg = wandb.config\n        # Construct a descriptive run name\n        run_name = (\n            f\"emb{cfg.embedding_dim}_hid{cfg.hidden_size}\"\n            f\"_enc{cfg.encoder_layers}_dec{cfg.decoder_layers}_\"\n            f\"{cfg.cell_type.lower()}_do{int(cfg.dropout_p*100)}\"\n            f\"_beam{cfg.beam_width}_tf{int(cfg.teacher_forcing_ratio*100)}\"\n        )\n        wandb.run.name = run_name\n\n        # Load data and build vocabularies\n        train_loader, val_loader, test_loader, src_size, tgt_size, \\\n        pad_idx, _, tgt_vocab = get_dataloaders(\n            dataset_dir, batch_size=64, build_vocab=True\n        )\n        idx2char = {i: ch for ch, i in tgt_vocab.items()}\n\n        # Initialize model, optimizer, loss, and early stopper\n        model = Seq2SeqModel(\n            src_vocab=src_size,\n            tgt_vocab=tgt_size,\n            embed_dim=cfg.embedding_dim,\n            hidden_dim=cfg.hidden_size,\n            enc_layers=cfg.encoder_layers,\n            dec_layers=cfg.decoder_layers,\n            cell_type=cfg.cell_type,\n            dropout=cfg.dropout_p,\n            bidirectional=False\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n        criterion = nn.NLLLoss(ignore_index=pad_idx)\n        stopper = EarlyStopper(patience=5)\n\n        best_val_acc = 0.0\n        # Training loop\n        for epoch in range(1, 11):\n            model.train()\n            epoch_loss = 0.0\n            for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False):\n                src, tgt = src.to(device), tgt.to(device)\n                optimizer.zero_grad()\n                outputs = model(src, tgt, teacher_forcing=cfg.teacher_forcing_ratio)\n                loss = criterion(outputs.view(-1, tgt_size), tgt.view(-1))\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            # Validation\n            model.eval()\n            correct, total = 0, 0\n            with torch.no_grad():\n                for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False):\n                    src, tgt = src.to(device), tgt.to(device)\n                    preds = model(src, tgt, teacher_forcing=0.0).argmax(dim=2)\n                    for p_seq, t_seq in zip(preds, tgt):\n                        pred_tokens = p_seq[1:][t_seq[1:] != pad_idx]\n                        true_tokens = t_seq[1:][t_seq[1:] != pad_idx]\n                        if torch.equal(pred_tokens, true_tokens):\n                            correct += 1\n                        total += 1\n            val_acc =100* correct / total\n            wandb.log({'epoch': epoch, 'train_loss': epoch_loss, 'val_accuracy': val_acc})\n\n            print(f\"Epoch {epoch} | Loss: {epoch_loss:.4f} | Val Acc: {val_acc:.4f}\")\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n            elif stopper.should_stop(val_acc):\n                print(\"Early stopping triggered.\")\n                break\n\n        # Final Test Evaluation\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for src, tgt in tqdm(test_loader, desc=\"Test Evaluation\", leave=False):\n                src, tgt = src.to(device), tgt.to(device)\n                preds = model(src, tgt, teacher_forcing=0.0).argmax(dim=2)\n                for p_seq, t_seq in zip(preds, tgt):\n                    pt = p_seq[1:][t_seq[1:] != pad_idx]\n                    tt = t_seq[1:][t_seq[1:] != pad_idx]\n                    if torch.equal(pt, tt):\n                        correct += 1\n                    total += 1\n        test_acc = 100* correct / total\n        print(f\"Final Test Accuracy: {test_acc:.4f}\")\n        wandb.log({'final_test_accuracy': test_acc})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:47.839681Z","iopub.execute_input":"2025-05-21T18:30:47.839942Z","iopub.status.idle":"2025-05-21T18:30:47.851710Z","shell.execute_reply.started":"2025-05-21T18:30:47.839923Z","shell.execute_reply":"2025-05-21T18:30:47.850895Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project='cs24m020_dl_a3_att')\nwandb.agent(sweep_id, function=train_run, count=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:30:50.754410Z","iopub.execute_input":"2025-05-21T18:30:50.754648Z","iopub.status.idle":"2025-05-21T18:36:06.522299Z","shell.execute_reply.started":"2025-05-21T18:30:50.754631Z","shell.execute_reply":"2025-05-21T18:36:06.521482Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 4yok39ch\nSweep URL: https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vsu5bz6p with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_183057-vsu5bz6p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/vsu5bz6p' target=\"_blank\">prime-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/vsu5bz6p' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/vsu5bz6p</a>"},"metadata":{}},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 1760.0845 | Val Acc: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Loss: 1128.9210 | Val Acc: 0.1239\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Loss: 884.5545 | Val Acc: 0.1641\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Loss: 803.2041 | Val Acc: 0.1804\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Loss: 745.5470 | Val Acc: 0.2118\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Loss: 710.2273 | Val Acc: 0.2299\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Loss: 691.8183 | Val Acc: 0.2393\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Loss: 672.3760 | Val Acc: 0.2480\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Loss: 652.8452 | Val Acc: 0.2643\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Loss: 640.2279 | Val Acc: 0.2616\n","output_type":"stream"},{"name":"stderr","text":"                                                                     ","output_type":"stream"},{"name":"stdout","text":"Final Test Accuracy: 0.0355\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.03554</td></tr><tr><td>train_loss</td><td>640.22792</td></tr><tr><td>val_accuracy</td><td>0.26159</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">emb64_hid64_enc2_dec2_rnn_do20_beam5_tf50</strong> at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/vsu5bz6p' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/vsu5bz6p</a><br> View project at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_183057-vsu5bz6p/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nm51vjma with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_183537-nm51vjma</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/nm51vjma' target=\"_blank\">smooth-sweep-2</a></strong> to <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/sweeps/4yok39ch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/nm51vjma' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/nm51vjma</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1 Training:  94%|█████████▍| 652/691 [00:22<00:01, 30.54it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 2009.0010 | Val Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training:  35%|███▌      | 244/691 [00:08<00:17, 26.04it/s]","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# === Dataset and Vocabulary ===\nCHAR2IDX_SRC = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3, **{ch: i + 4 for i, ch in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}}\nIDX2CHAR_SRC = {i: ch for ch, i in CHAR2IDX_SRC.items()}\n\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\nIDX2CHAR_TGT = {i: ch for ch, i in tgt_vocab.items()}\n\n# === Model Instantiation ===\nbest_model = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=256,\n    encoder_layers=3,\n    decoder_layers=1,\n    cell_type='LSTM',\n    dropout_p=0.4,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = optim.Adam(best_model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\n\n# === Training Routine ===\nbest_val_acc = 0.0\nfor epoch in range(1, 11):\n    best_model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch} - Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = best_model(src, tgt, teacher_forcing_ratio=1.0)\n        loss = criterion(outputs.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # === Validation ===\n    best_model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch} - Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            outputs = best_model(src, tgt, teacher_forcing_ratio=0.0)\n            predictions = outputs.argmax(dim=2)\n            for pred_seq, true_seq in zip(predictions, tgt):\n                pred_clean = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_clean = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_clean, true_clean):\n                    correct += 1\n                total += 1\n\n    val_acc = correct / total\n    print(f\"Epoch {epoch} | Loss: {total_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping criteria met.\")\n        break\n\n# === Test Evaluation ===\ncorrect, total = 0, 0\npredictions_all, targets_all = [], []\n\nbest_model.eval()\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Testing\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        outputs = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        predictions = outputs.argmax(dim=2)\n\n        for pred_seq, true_seq in zip(predictions, tgt):\n            pred_clean = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_clean = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_clean, true_clean):\n                correct += 1\n            total += 1\n            predictions_all.append(pred_clean)\n            targets_all.append(true_clean)\n\nprint(f\"\\nFinal Exact Match Accuracy on Test Set: {correct / total:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Sample Predictions ===\nroman_inputs = []\nwith open(os.path.join(BASE_DIR, 'te.translit.sampled.test.tsv'), encoding='utf-8') as test_file:\n    for line in test_file:\n        _, romanized_word, _ = line.strip().split()\n        roman_inputs.append(romanized_word)\n\npredicted_samples = []\nbest_model.eval()\nwith torch.no_grad():\n    for batch_idx, (src_batch, tgt_batch) in enumerate(test_loader):\n        src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n        logits = best_model(src_batch, tgt_batch, teacher_forcing_ratio=0.0)\n        decoded = logits.argmax(dim=2)\n\n        for i in range(src_batch.size(0)):\n            predicted = ''.join(\n                IDX2CHAR_TGT[idx.item()] for idx in decoded[i][1:] if idx.item() != pad_idx\n            )\n            ground_truth = ''.join(\n                IDX2CHAR_TGT[idx.item()] for idx in tgt_batch[i][1:] if idx.item() != pad_idx\n            )\n            predicted_samples.append({\n                'Romanized Input': roman_inputs[batch_idx * src_batch.size(0) + i],\n                'True Output': ground_truth,\n                'Model Output': predicted\n            })\n\n# === Display Sample Results ===\npreview_samples = random.sample(predicted_samples, min(10, len(predicted_samples)))\ndf = pd.DataFrame(preview_samples)\nprint(df.to_markdown(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T18:36:17.895040Z","iopub.execute_input":"2025-05-21T18:36:17.895317Z","iopub.status.idle":"2025-05-21T18:36:17.938319Z","shell.execute_reply.started":"2025-05-21T18:36:17.895296Z","shell.execute_reply":"2025-05-21T18:36:17.936921Z"}},"outputs":[{"name":"stderr","text":"Epoch 2 Training:  36%|███▌      | 247/691 [00:08<00:17, 25.76it/s]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/974980266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mromanized_test_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hi.translit.sampled.test.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mhindi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mromanized_test_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BASE_DIR' is not defined"],"ename":"NameError","evalue":"name 'BASE_DIR' is not defined","output_type":"error"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Loss: 1821.2137 | Val Acc: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training:  41%|████▏     | 286/691 [00:10<00:14, 27.85it/s]","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# === Display Sample Results with Highlighting ===\ndef style_mismatches(row):\n    styling = [''] * len(row)\n    try:\n        col_names = list(row.index)\n        idx = col_names.index('Model Output')\n        if row['Model Output'] == row['True Output']:\n            styling[idx] = 'background-color: #d4edda; font-weight: bold;'\n        else:\n            styling[idx] = 'background-color: #f8d7da; font-weight: bold;'\n    except ValueError:\n        pass\n    return styling\n\nsample_subset = pd.DataFrame(random.sample(predicted_samples, min(10, len(predicted_samples))))\nstyled_df = (\n    sample_subset.style\n        .apply(style_mismatches, axis=1)\n        .set_table_styles([\n            {'selector': 'td, th', 'props': [('text-align', 'center'), ('padding', '6px')]},\n            {'selector': 'th', 'props': [('background-color', '#4F81BD'), ('color', 'white'), ('font-weight', 'bold'), ('padding', '8px')]}\n        ])\n        .set_caption(\"✨ Sample Transliteration Predictions (Green = Correct, Red = Wrong) ✨\")\n)\n\n# Display in Jupyter Notebook or similar environment\ndisplay(styled_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:53:43.335132Z","iopub.execute_input":"2025-05-21T14:53:43.335442Z","iopub.status.idle":"2025-05-21T14:53:43.346663Z","shell.execute_reply.started":"2025-05-21T14:53:43.335418Z","shell.execute_reply":"2025-05-21T14:53:43.346091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x79535f764d90>","text/html":"<style type=\"text/css\">\n#T_fbcf8 td {\n  text-align: center;\n  padding: 6px;\n}\n#T_fbcf8  th {\n  text-align: center;\n  padding: 6px;\n}\n#T_fbcf8 th {\n  background-color: #4F81BD;\n  color: white;\n  font-weight: bold;\n  padding: 8px;\n}\n#T_fbcf8_row0_col2, #T_fbcf8_row2_col2, #T_fbcf8_row3_col2, #T_fbcf8_row4_col2, #T_fbcf8_row5_col2, #T_fbcf8_row6_col2, #T_fbcf8_row8_col2, #T_fbcf8_row9_col2 {\n  background-color: #f8d7da;\n  font-weight: bold;\n}\n#T_fbcf8_row1_col2, #T_fbcf8_row7_col2 {\n  background-color: #c8e6c9;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_fbcf8\">\n  <caption>✨ Sample Transliteration Predictions (Green = Correct, Red = Wrong) ✨</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_fbcf8_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n      <th id=\"T_fbcf8_level0_col1\" class=\"col_heading level0 col1\" >True Hindi</th>\n      <th id=\"T_fbcf8_level0_col2\" class=\"col_heading level0 col2\" >Predicted Hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_fbcf8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_fbcf8_row0_col0\" class=\"data row0 col0\" >manchit</td>\n      <td id=\"T_fbcf8_row0_col1\" class=\"data row0 col1\" >मंचित<eos></td>\n      <td id=\"T_fbcf8_row0_col2\" class=\"data row0 col2\" >माचित<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_fbcf8_row1_col0\" class=\"data row1 col0\" >sanskrit</td>\n      <td id=\"T_fbcf8_row1_col1\" class=\"data row1 col1\" >संस्कृत<eos></td>\n      <td id=\"T_fbcf8_row1_col2\" class=\"data row1 col2\" >संस्कृत<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_fbcf8_row2_col0\" class=\"data row2 col0\" >girijagharon</td>\n      <td id=\"T_fbcf8_row2_col1\" class=\"data row2 col1\" >गिरजाघरों<eos></td>\n      <td id=\"T_fbcf8_row2_col2\" class=\"data row2 col2\" >गिरिजाघरों</td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_fbcf8_row3_col0\" class=\"data row3 col0\" >mem</td>\n      <td id=\"T_fbcf8_row3_col1\" class=\"data row3 col1\" >मेम<eos></td>\n      <td id=\"T_fbcf8_row3_col2\" class=\"data row3 col2\" >म<eos><eos><eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_fbcf8_row4_col0\" class=\"data row4 col0\" >uniyal</td>\n      <td id=\"T_fbcf8_row4_col1\" class=\"data row4 col1\" >उनियाल<eos></td>\n      <td id=\"T_fbcf8_row4_col2\" class=\"data row4 col2\" >यूनियल<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_fbcf8_row5_col0\" class=\"data row5 col0\" >vishhin</td>\n      <td id=\"T_fbcf8_row5_col1\" class=\"data row5 col1\" >विषहीन<eos></td>\n      <td id=\"T_fbcf8_row5_col2\" class=\"data row5 col2\" >विशियों</td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_fbcf8_row6_col0\" class=\"data row6 col0\" >majin</td>\n      <td id=\"T_fbcf8_row6_col1\" class=\"data row6 col1\" >माजिन<eos></td>\n      <td id=\"T_fbcf8_row6_col2\" class=\"data row6 col2\" >मीन<eos><eos><eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_fbcf8_row7_col0\" class=\"data row7 col0\" >bhairavnath</td>\n      <td id=\"T_fbcf8_row7_col1\" class=\"data row7 col1\" >भैरवनाथ<eos></td>\n      <td id=\"T_fbcf8_row7_col2\" class=\"data row7 col2\" >भैरवनाथ<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_fbcf8_row8_col0\" class=\"data row8 col0\" >bokaro</td>\n      <td id=\"T_fbcf8_row8_col1\" class=\"data row8 col1\" >बोकारो<eos></td>\n      <td id=\"T_fbcf8_row8_col2\" class=\"data row8 col2\" >बोकरों<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_fbcf8_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_fbcf8_row9_col0\" class=\"data row9 col0\" >amitao</td>\n      <td id=\"T_fbcf8_row9_col1\" class=\"data row9 col1\" >अमिताव<eos></td>\n      <td id=\"T_fbcf8_row9_col2\" class=\"data row9 col2\" >अमीताओं</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"wandb.init(project=\"cs24m020_dl_a3_att\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:15:32.816557Z","iopub.execute_input":"2025-05-21T15:15:32.817148Z","iopub.status.idle":"2025-05-21T15:15:39.899893Z","shell.execute_reply.started":"2025-05-21T15:15:32.817128Z","shell.execute_reply":"2025-05-21T15:15:39.899116Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_151532-0807oez7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/0807oez7' target=\"_blank\">upbeat-lake-124</a></strong> to <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/0807oez7' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/0807oez7</a>"},"metadata":{}},{"execution_count":40,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_att/runs/0807oez7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79530016b590>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"# Set the font family to Noto Sans Telugu\nrcParams['font.family'] = ['Noto Sans', 'Noto Sans Hindi', 'sans-serif']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:15:39.901041Z","iopub.execute_input":"2025-05-21T15:15:39.901270Z","iopub.status.idle":"2025-05-21T15:15:39.905418Z","shell.execute_reply.started":"2025-05-21T15:15:39.901252Z","shell.execute_reply":"2025-05-21T15:15:39.904711Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# === Attention Heatmap Logging ===\nwandb.init(project=\"cs24m020_dl_a3_att\")\nrcParams['font.family'] = ['Noto Sans', 'Noto Sans Hindi', 'sans-serif']\nos.makedirs(\"predictions_vanilla/attention_maps\", exist_ok=True)\n\nbest_model.eval()\nattention_imgs = []\nmax_samples = 12\ncollected = 0\n\nwith torch.no_grad():\n    for src, tgt in test_loader:\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        logits, attention = best_model(src, tgt, return_attention=True)\n\n        for i in range(src.size(0)):\n            src_seq = [IDX2CHAR_SRC[idx.item()] for idx in src[i] if idx.item() != pad_idx]\n            true_seq = [IDX2CHAR_TGT[idx.item()] for idx in tgt[i][1:] if idx.item() != pad_idx]\n            pred_seq = logits.argmax(dim=2)[i][1:len(true_seq)+1]\n            pred_chars = [IDX2CHAR_TGT[idx.item()] for idx in pred_seq]\n            attn_matrix = attention[i][:len(pred_chars), :len(src_seq)]\n\n            fig, ax = plt.subplots(figsize=(6, 4))\n            sns.heatmap(attn_matrix.cpu().numpy(),\n                        xticklabels=src_seq,\n                        yticklabels=pred_chars,\n                        cmap='coolwarm',\n                        cbar=False,\n                        linewidths=0.5,\n                        ax=ax)\n            ax.set_xlabel(\"Input\")\n            ax.set_ylabel(\"Predicted Output (Hindi)\")\n            ax.set_title(f\"Attention Heatmap {collected + 1}\")\n            plt.tight_layout()\n\n            save_path = f\"predictions_vanilla/attention_maps/sample_{collected + 1}.png\"\n            fig.savefig(save_path)\n            plt.close(fig)\n\n            attention_imgs.append(wandb.Image(save_path, caption=f\"Sample {collected + 1}\"))\n            collected += 1\n            if collected >= max_samples:\n                break\n        if collected >= max_samples:\n            break\n\nwandb.log({\"attention_maps\": attention_imgs})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:15:39.906108Z","iopub.execute_input":"2025-05-21T15:15:39.906382Z","iopub.status.idle":"2025-05-21T15:15:41.304098Z","shell.execute_reply.started":"2025-05-21T15:15:39.906358Z","shell.execute_reply":"2025-05-21T15:15:41.303381Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2381 (\\N{DEVANAGARI SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2368 (\\N{DEVANAGARI VOWEL SIGN II}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2381 (\\N{DEVANAGARI SIGN VIRAMA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2368 (\\N{DEVANAGARI VOWEL SIGN II}) missing from current font.\n  fig.savefig(path)\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2381 (\\N{DEVANAGARI SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2375 (\\N{DEVANAGARI VOWEL SIGN E}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2332 (\\N{DEVANAGARI LETTER JA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2368 (\\N{DEVANAGARI VOWEL SIGN II}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2381 (\\N{DEVANAGARI SIGN VIRAMA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2375 (\\N{DEVANAGARI VOWEL SIGN E}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2332 (\\N{DEVANAGARI LETTER JA}) missing from current font.\n  fig.savefig(path)\n/tmp/ipykernel_35/3627413280.py:45: UserWarning: Glyph 2368 (\\N{DEVANAGARI VOWEL SIGN II}) missing from current font.\n  fig.savefig(path)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"df = pd.DataFrame(samples)\noutput_dir = '/kaggle/working/predictions-vanilla'\nos.makedirs(output_dir, exist_ok=True) \ncsv_path = '/kaggle/working/predictions-vanilla/output.csv'\ndf.to_csv(csv_path, index=False, encoding='utf-8-sig')\n\nprint(f\"\\n✅ Saved all predictions to: {csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:15:41.305588Z","iopub.execute_input":"2025-05-21T15:15:41.305828Z","iopub.status.idle":"2025-05-21T15:15:41.325932Z","shell.execute_reply.started":"2025-05-21T15:15:41.305812Z","shell.execute_reply":"2025-05-21T15:15:41.325272Z"}},"outputs":[{"name":"stdout","text":"\n✅ Saved all predictions to: /kaggle/working/predictions-vanilla/output.csv\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# === Interactive HTML Attention Viewer ===\nhtml_snippets = []\nfor idx, (src_tok, pred_tok, weights) in enumerate(random_samples):\n    js_input = json.dumps(src_tok, ensure_ascii=False)\n    js_output = json.dumps(pred_tok, ensure_ascii=False)\n    js_matrix = json.dumps(weights)\n\n    snippet = f'''\n    <div class=\"sample-block\">\n      <h2>Sample {idx + 1}</h2>\n      <div class=\"token-label\">Input (Romanized):</div>\n      <div class=\"token-container input-tokens\" id=\"input-{idx}\"></div>\n      <div class=\"token-label\">Predicted Hindi Output:</div>\n      <div class=\"token-container output-tokens\" id=\"output-{idx}\"></div>\n      <script>\n        const input_{idx} = {js_input};\n        const output_{idx} = {js_output};\n        const attn_{idx} = {js_matrix};\n\n        const input_div_{idx} = d3.select(\"#input-{idx}\");\n        const output_div_{idx} = d3.select(\"#output-{idx}\");\n\n        input_{idx}.forEach((tok, i) => {{\n          input_div_{idx}.append(\"span\")\n            .attr(\"class\", \"token input\")\n            .attr(\"id\", \"tok-in-{idx}-\" + i)\n            .text(tok);\n        }});\n\n        output_{idx}.forEach((tok, i) => {{\n          output_div_{idx}.append(\"span\")\n            .attr(\"class\", \"token output\")\n            .attr(\"title\", \"Hover to see alignment\")\n            .text(tok)\n            .on(\"mouseover\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#f9f9f9\");\n              attn_{idx}[i].forEach((val, j) => {{\n                const color = d3.interpolateBlues(val);\n                d3.select(\"#tok-in-{idx}-\" + j).style(\"background-color\", color);\n              }});\n            }})\n            .on(\"mouseout\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#f9f9f9\");\n            }});\n        }});\n      </script>\n    </div>\n    '''\n    html_snippets.append(snippet)\n\nfull_html = f'''\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Seq2Seq Attention Viewer</title>\n  <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n  <style>\n    body {{ font-family: 'Segoe UI', sans-serif; background: #f2f4f8; margin: 40px; color: #333; }}\n    .sample-block {{ background: #fff; padding: 20px; margin-bottom: 40px; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.06); }}\n    .token-label {{ font-weight: bold; font-size: 16px; margin-top: 10px; color: #555; }}\n    .token-container {{ display: flex; flex-wrap: wrap; margin: 10px 0; }}\n    .token {{ padding: 8px 12px; margin: 4px; border-radius: 6px; font-size: 18px; cursor: pointer; border: 1px solid #ccc; background-color: #f9f9f9; transition: background-color 0.3s; }}\n    .token.output {{ background-color: #e8f0fe; border-color: #a0c4ff; }}\n  </style>\n</head>\n<body>\n  <h1>Attention Visualizations for 10 Random Samples</h1>\n  {''.join(html_snippets)}\n</body>\n</html>\n'''\n\nwandb.log({\"att_visual\": wandb.Html(full_html)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:15:42.059062Z","iopub.execute_input":"2025-05-21T15:15:42.059363Z","iopub.status.idle":"2025-05-21T15:15:42.070649Z","shell.execute_reply.started":"2025-05-21T15:15:42.059342Z","shell.execute_reply":"2025-05-21T15:15:42.070023Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import wandb\n\n# 1. Start your run\nrun = wandb.init(\n    project=\"my-project\",\n    job_type=\"html-report\",\n    name=\"attention-visualization\"\n)\n\n# 2. Write the HTML string to a file\nwith open(\"att_visual.html\", \"w\", encoding=\"utf-8\") as f:\n    f.write(full_html)\n\n# 3. (Optional) Log it for in‐UI preview this run\nrun.log({\"att_visual_preview\": wandb.Html(\"att_visual.html\")})\n\n# 4. Create an Artifact and attach the file\nartifact = wandb.Artifact(\n    name=\"attention-viz-report\",\n    type=\"html-report\",\n    description=\"Permanent HTML attention visualization\"\n)\nartifact.add_file(\"att_visual.html\")\n\n# 5. Log the Artifact so it never expires\nrun.log_artifact(artifact)\n\n# 6. Finish the run\nrun.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:18:11.232470Z","iopub.execute_input":"2025-05-21T15:18:11.233032Z","iopub.status.idle":"2025-05-21T15:18:22.082298Z","shell.execute_reply.started":"2025-05-21T15:18:11.233009Z","shell.execute_reply":"2025-05-21T15:18:22.081723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_151811-or9a69r7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project/runs/or9a69r7' target=\"_blank\">attention-visualization</a></strong> to <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project/runs/or9a69r7' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project/runs/or9a69r7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attention-visualization</strong> at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project/runs/or9a69r7' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project/runs/or9a69r7</a><br> View project at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/my-project</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_151811-or9a69r7/logs</code>"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}