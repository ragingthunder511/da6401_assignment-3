{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11873479,"sourceType":"datasetVersion","datasetId":7461882}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ─── Python Built-ins & Utilities ─────────────────────────────────────────────\nimport os                      # filesystem operations\nimport re                      # regex handling\nimport random                  # random number generation\n\n# ─── Data Processing & Tracking ───────────────────────────────────────────────\nimport pandas                  # tabular data manipulation\nfrom tqdm.auto import tqdm     # progress bars\nimport wandb                   # experiment logging\n\n# ─── PyTorch Core ─────────────────────────────────────────────────────────────\nimport torch                   # tensor operations & autograd\nimport torch.nn as nn          # neural network layers\nimport torch.optim as optim    # optimizers\nimport torch.nn.functional as F  # activation & loss functions\nfrom torch.utils.data import Dataset, DataLoader  # data pipeline\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:05.272199Z","iopub.execute_input":"2025-05-20T19:17:05.272489Z","iopub.status.idle":"2025-05-20T19:17:05.279380Z","shell.execute_reply.started":"2025-05-20T19:17:05.272466Z","shell.execute_reply":"2025-05-20T19:17:05.278344Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ─── Experiment Tracker Setup ─────────────────────────────────────────────────\n# Ensure a WANDB key is present, else apply the embedded fallback\nWANDB_KEY = \"8f2f82255a6e5ea16321da3895ae6b00d50eb5b5\"\nos.environ.setdefault(\"WANDB_API_KEY\", WANDB_KEY)\ntry:\n    # ‘relogin=True’ forces a fresh session if needed\n    wandb.login(key=WANDB_KEY, relogin=True)\nexcept wandb.errors.UsageError:\n    # Already authenticated or bad key—ignore quietly\n    pass\n\n# ─── Reproducibility & Hardware Choice ────────────────────────────────────────\nSEED = 42\nrandom.seed(SEED)           # Python RNG\ntorch.manual_seed(SEED)     # PyTorch RNG\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ─── Data Directory Specification ─────────────────────────────────────────────\n# Path to the Dakshina lexicon files; swap 'hi' with any other language code as needed\nLEXICON_ROOT = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons\"\n# e.g., for Marathi:\n# LEXICON_ROOT = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:09.234417Z","iopub.execute_input":"2025-05-20T19:17:09.234987Z","iopub.status.idle":"2025-05-20T19:17:09.247129Z","shell.execute_reply.started":"2025-05-20T19:17:09.234959Z","shell.execute_reply":"2025-05-20T19:17:09.245988Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ─── Character-Level Embeddings ───────────────────────────────────────────────\nclass CharEmbedder(nn.Module):\n    \"\"\"Maps character indices to dense vectors.\"\"\"\n    def __init__(self, vocab_size: int, emb_dim: int):\n        super().__init__()\n        self.lookup = nn.Embedding(vocab_size, emb_dim)\n\n    def forward(self, indices: torch.Tensor) -> torch.Tensor:\n        # input: [batch, seq_len] → output: [batch, seq_len, emb_dim]\n        return self.lookup(indices)\n\n\n# ─── Sequence Encoder (RNN/GRU/LSTM) ──────────────────────────────────────────\nclass SeqEncoder(nn.Module):\n    \"\"\"\n    Encodes token sequences into contextual representations.\n    \n    Args:\n      vocab_size: size of input token set\n      hid_dim: size of hidden state\n      emb_dim: size of embedding vectors\n      layers: number of recurrent layers\n      rnn_type: 'GRU', 'LSTM', or 'RNN'\n      drop: dropout probability between RNN layers\n      bidir: bidirectional flag\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hid_dim: int,\n        emb_dim: int,\n        layers: int = 1,\n        rnn_type: str = \"GRU\",\n        drop: float = 0.1,\n        bidir: bool = False,\n    ):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.num_dirs = 2 if bidir else 1\n\n        # embed → dropout → RNN\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.drop = nn.Dropout(drop)\n        core_drop = drop if layers > 1 else 0.0\n\n        rnn_cls = {\n            \"GRU\": nn.GRU,\n            \"LSTM\": nn.LSTM,\n            \"RNN\": nn.RNN\n        }[rnn_type]\n        rnn_kwargs = dict(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=layers,\n            dropout=core_drop,\n            bidirectional=bidir,\n            batch_first=True\n        )\n        # For vanilla RNN, specify nonlinearity\n        if rnn_type == \"RNN\":\n            rnn_kwargs[\"nonlinearity\"] = \"tanh\"\n\n        self.rnn = rnn_cls(**rnn_kwargs)\n\n    def forward(self, tokens: torch.Tensor):\n        # tokens: [batch, seq_len]\n        emb = self.drop(self.embed(tokens))  # [batch, seq_len, emb_dim]\n        outputs, state = self.rnn(emb)\n        return outputs, state  # outputs: all timesteps; state: final hidden (and cell)\n\n\n# ─── Token-Level Decoder (Stepwise) ──────────────────────────────────────────\nclass SeqDecoder(nn.Module):\n    \"\"\"\n    Generates tokens one step at a time.\n    \n    Args:\n      out_size: target vocabulary size\n      hid_dim: hidden state dimension\n      emb_dim: embedding vector size\n      layers: number of RNN layers\n      rnn_type: 'GRU', 'LSTM', or 'RNN'\n      drop: dropout probability before and after RNN\n    \"\"\"\n    def __init__(\n        self,\n        out_size: int,\n        hid_dim: int,\n        emb_dim: int,\n        layers: int = 1,\n        rnn_type: str = \"GRU\",\n        drop: float = 0.1,\n    ):\n        super().__init__()\n        # embed → RNN → dropout → linear → log-softmax\n        self.embed = nn.Embedding(out_size, emb_dim)\n        self.rnn_drop = nn.Dropout(drop)\n        core_drop = drop if layers > 1 else 0.0\n\n        rnn_cls = {\n            \"GRU\": nn.GRU,\n            \"LSTM\": nn.LSTM,\n            \"RNN\": nn.RNN\n        }[rnn_type]\n        rnn_kwargs = dict(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=layers,\n            dropout=core_drop,\n            batch_first=True\n        )\n        if rnn_type == \"RNN\":\n            rnn_kwargs[\"nonlinearity\"] = \"tanh\"\n\n        self.rnn = rnn_cls(**rnn_kwargs)\n        self.out_drop = nn.Dropout(core_drop)\n        self.project = nn.Linear(hid_dim, out_size)\n\n    def forward(self, input_tok: torch.Tensor, prev_state):\n        \"\"\"\n        Args:\n          input_tok: [batch, 1] token indices for current timestep\n          prev_state: previous hidden (and cell) state\n        Returns:\n          log_probs: [batch, out_size]\n          new_state: updated hidden (and cell) state\n        \"\"\"\n        emb = self.embed(input_tok)           # [batch, 1, emb_dim]\n        emb = self.rnn_drop(emb)\n        rnn_out, new_state = self.rnn(emb, prev_state)\n        dropped = self.out_drop(rnn_out[:, 0, :])  # take timestep dim\n        logits = self.project(dropped)\n        return F.log_softmax(logits, dim=-1), new_state\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:15.214149Z","iopub.execute_input":"2025-05-20T19:17:15.214403Z","iopub.status.idle":"2025-05-20T19:17:15.230574Z","shell.execute_reply.started":"2025-05-20T19:17:15.214382Z","shell.execute_reply":"2025-05-20T19:17:15.229409Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#  Perform beam search decoding with the trained seq2seq model.\ndef beam_decode(\n    model,\n    src_batch: torch.Tensor,\n    start_idx: int,\n    end_idx: int,\n    max_len: int = 30,\n    beam_size: int = 3,\n    device: torch.device = DEVICE,\n):\n    # ─── Prepare Model & Encode ──────────────────────────────────────────────────\n    model.eval()\n    with torch.no_grad():\n        enc_outs, enc_state = model.encoder(src_batch.to(device))\n\n        # ─── Initialize Decoder State ──────────────────────────────────────────────\n        if model.bidirectional:\n            # Merge forward/backward layers\n            def _merge(h, c=None):\n                L = model.encoder.num_layers\n                h_new = torch.zeros(L, 1, model.decoder.hidden_size, device=device)\n                c_new = None if c is None else torch.zeros_like(h_new)\n                for i in range(L):\n                    f, b = h[2*i], h[2*i+1]\n                    h_new[i] = model.hidden_transform(torch.cat((f, b), dim=1))\n                    if c is not None:\n                        fc, bc = c[2*i], c[2*i+1]\n                        c_new[i] = model.hidden_transform(torch.cat((fc, bc), dim=1))\n                return (h_new, c_new) if c is not None else h_new\n\n            if isinstance(enc_state, tuple):  # LSTM\n                dec_state = _merge(*enc_state)\n            else:                             # GRU/RNN\n                dec_state = _merge(enc_state)\n        else:\n            dec_state = enc_state\n\n        # ─── Beam Search Setup ─────────────────────────────────────────────────────\n        beams = [([start_idx], 0.0, dec_state)]  # (tokens, score, state)\n        completed = []\n\n        # ─── Expand Beams ─────────────────────────────────────────────────────────\n        for _ in range(max_len):\n            candidates = []\n            for seq, score, state in beams:\n                if seq[-1] == end_idx:\n                    completed.append((seq, score))\n                    continue\n                inp = torch.tensor([[seq[-1]]], device=device)\n                logp, next_state = model.decoder(inp, state)\n                topv, topi = torch.topk(logp.squeeze(0), beam_size)\n                for v, i in zip(topv, topi):\n                    candidates.append((seq + [i.item()], score + v.item(), next_state))\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            if not beams:\n                break\n\n        # ─── Finalize Outputs ──────────────────────────────────────────────────────\n        for seq, score, _ in beams:\n            if seq[-1] == end_idx:\n                completed.append((seq, score))\n        if not completed:\n            completed = [(s, sc) for s, sc, _ in beams]\n\n        return sorted(completed, key=lambda x: x[1], reverse=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:17.991126Z","iopub.execute_input":"2025-05-20T19:17:17.991848Z","iopub.status.idle":"2025-05-20T19:17:18.005426Z","shell.execute_reply.started":"2025-05-20T19:17:17.991818Z","shell.execute_reply":"2025-05-20T19:17:18.004228Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport random\n\n# ─── End-to-End Seq2Seq Wrapper ────────────────────────────────────────────────\nclass Seq2SeqModel(nn.Module):\n    \"\"\"\n    Combines encoder and decoder for sequence-to-sequence tasks.\n    \n    Args:\n      src_vocab: source vocabulary size\n      tgt_vocab: target vocabulary size\n      emb_dim: embedding dimension for both encoder & decoder\n      hid_dim: hidden state size (must match between enc/dec)\n      enc_layers: number of encoder RNN layers\n      dec_layers: number of decoder RNN layers\n      rnn_type: 'GRU', 'LSTM', or 'RNN'\n      drop_p: dropout rate on embeddings & RNN layers\n      bidir_enc: if True, encoder is bidirectional\n    \"\"\"\n    def __init__(\n        self,\n        src_vocab: int,\n        tgt_vocab: int,\n        emb_dim: int = 256,\n        hid_dim: int = 256,\n        enc_layers: int = 1,\n        dec_layers: int = 1,\n        rnn_type: str = \"GRU\",\n        drop_p: float = 0.2,\n        bidir_enc: bool = False,\n    ):\n        super().__init__()\n        # ─── Encoder Setup ───────────────────────────────────────────────────────\n        self.encoder = SeqEncoder(\n            vocab_size=src_vocab,\n            hid_dim=hid_dim,\n            emb_dim=emb_dim,\n            layers=enc_layers,\n            rnn_type=rnn_type,\n            drop=drop_p,\n            bidir=bidir_enc,\n        )\n        self.bidir = bidir_enc\n        # transform bilinear hidden → decoder size if bidirectional\n        if bidir_enc:\n            self.hidden_proj = nn.Linear(hid_dim * 2, hid_dim)\n        # ─── Decoder Setup ───────────────────────────────────────────────────────\n        self.decoder = SeqDecoder(\n            out_size=tgt_vocab,\n            hid_dim=hid_dim,\n            emb_dim=emb_dim,\n            layers=dec_layers,\n            rnn_type=rnn_type,\n            drop=drop_p,\n        )\n        self.rnn_type = rnn_type\n\n    # ─── Align Hidden Layers ────────────────────────────────────────────────────\n    def _match_layers(self, h: torch.Tensor, bsz: int):\n        # trim or pad hidden states to decoder layer count\n        dl = self.decoder.rnn.num_layers\n        if h.size(0) > dl:\n            return h[:dl]\n        if h.size(0) < dl:\n            pad = torch.zeros(dl - h.size(0), bsz, h.size(2), device=h.device)\n            return torch.cat([h, pad], dim=0)\n        return h\n\n    def forward(self, src: torch.Tensor, tgt: torch.Tensor, tf_ratio: float = 0.5):\n        # ─── Prep & Encode ───────────────────────────────────────────────────────\n        bsz, tgt_len = src.size(0), tgt.size(1)\n        out_vocab = self.decoder.project.out_features if hasattr(self.decoder, 'project') else self.decoder.project.weight.size(0)\n        outputs = torch.zeros(bsz, tgt_len, out_vocab, device=src.device)\n\n        enc_outs, enc_state = self.encoder(src)\n        # ─── Init Decoder State ──────────────────────────────────────────────────\n        if self.bidir:\n            # combine forward/back encoder states\n            if self.rnn_type == \"LSTM\":\n                h_n, c_n = enc_state\n                h_dec = torch.zeros(self.decoder.rnn.num_layers, bsz, self.decoder.rnn.hidden_size, device=src.device)\n                c_dec = torch.zeros_like(h_dec)\n                for i in range(self.decoder.rnn.num_layers):\n                    layer = min(i, self.encoder.rnn.num_layers - 1)\n                    h_cat = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    c_cat = torch.cat((c_n[2*layer], c_n[2*layer+1]), dim=1)\n                    h_dec[i] = self.hidden_proj(h_cat)\n                    c_dec[i] = self.hidden_proj(c_cat)\n                dec_state = (h_dec, c_dec)\n            else:\n                h_n = enc_state\n                h_dec = torch.zeros(self.decoder.rnn.num_layers, bsz, self.decoder.rnn.hidden_size, device=src.device)\n                for i in range(self.decoder.rnn.num_layers):\n                    layer = min(i, self.encoder.rnn.num_layers - 1)\n                    h_cat = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    h_dec[i] = self.hidden_proj(h_cat)\n                dec_state = h_dec\n        else:\n            # match layer counts for unidirectional\n            if self.rnn_type == \"LSTM\":\n                h, c = enc_state\n                dec_state = (\n                    self._match_layers(h, bsz),\n                    self._match_layers(c, bsz),\n                )\n            else:\n                dec_state = self._match_layers(enc_state, bsz)\n\n        # ─── Decode with Teacher Forcing ──────────────────────────────────────────\n        input_tok = tgt[:, 0].unsqueeze(1)  # first token (<sos>)\n        for t in range(1, tgt_len):\n            probs, dec_state = self.decoder(input_tok, dec_state)\n            outputs[:, t, :] = probs\n            # decide next input\n            top1 = probs.argmax(1).unsqueeze(1)\n            if random.random() < tf_ratio:\n                input_tok = tgt[:, t].unsqueeze(1)\n            else:\n                input_tok = top1\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:18.335640Z","iopub.execute_input":"2025-05-20T19:17:18.336360Z","iopub.status.idle":"2025-05-20T19:17:18.352490Z","shell.execute_reply.started":"2025-05-20T19:17:18.336321Z","shell.execute_reply":"2025-05-20T19:17:18.351347Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# ─── Dataset for Romanized ↔ Native Transliterations ───────────────────────────\nclass TranslitDataset(Dataset):\n    \"\"\"\n    Reads tab-separated transliteration pairs and optionally builds vocabularies.\n    \"\"\"\n    def __init__(self, filepath, src_vocab=None, tgt_vocab=None, build_vocab=False):\n        self.pairs = []\n        # load all (src, tgt) pairs\n        with open(filepath, encoding=\"utf-8\") as f:\n            for line in f:\n                parts = line.strip().split(\"\\t\")\n                if len(parts) >= 2:\n                    tgt, src = parts[0], parts[1]\n                    self.pairs.append((src, tgt))\n\n        # build or assign vocabularies\n        if build_vocab:\n            self.src_vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n            self.tgt_vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n            for src, tgt in self.pairs:\n                for ch in src:\n                    self.src_vocab.setdefault(ch, len(self.src_vocab))\n                for ch in tgt:\n                    self.tgt_vocab.setdefault(ch, len(self.tgt_vocab))\n        else:\n            assert src_vocab and tgt_vocab, \"Provide vocabularies if not building.\"\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        src, tgt = self.pairs[idx]\n        # convert chars → indices with <unk> fallback\n        src_ids = [self.src_vocab.get(c, self.src_vocab[\"<unk>\"]) for c in src]\n        tgt_ids = (\n            [self.tgt_vocab[\"<sos>\"]]\n            + [self.tgt_vocab.get(c, self.tgt_vocab[\"<unk>\"]) for c in tgt]\n            + [self.tgt_vocab[\"<eos>\"]]\n        )\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n\n\n# ─── Batch Collation & Padding ─────────────────────────────────────────────────\ndef pad_collate(batch):\n    \"\"\"\n    Pads source and target sequences in batch to their max lengths.\n    Returns:\n      padded_src: (bs, max_src), padded_tgt: (bs, max_tgt)\n    \"\"\"\n    src_seqs, tgt_seqs = zip(*batch)\n    max_src = max(len(s) for s in src_seqs)\n    max_tgt = max(len(t) for t in tgt_seqs)\n\n    pad_idx = 0\n    padded_src = torch.full((len(batch), max_src), pad_idx, dtype=torch.long)\n    padded_tgt = torch.full((len(batch), max_tgt), pad_idx, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n        padded_src[i, : len(s)] = s\n        padded_tgt[i, : len(t)] = t\n\n    return padded_src, padded_tgt\n\n\n# ─── DataLoader Factory ─────────────────────────────────────────────────────────\ndef get_dataloaders(root_dir, batch_size: int, build_vocab: bool = False):\n    \"\"\"\n    Constructs train/val/test DataLoaders and returns vocab info.\n    Returns:\n      train_loader, val_loader, test_loader,\n      src_vocab_size, tgt_vocab_size, pad_index, src_vocab, tgt_vocab\n    \"\"\"\n    # file paths for Hindi transliteration splits\n    train_file = os.path.join(root_dir, \"hi.translit.sampled.train.tsv\")\n    val_file   = os.path.join(root_dir, \"hi.translit.sampled.dev.tsv\")\n    test_file  = os.path.join(root_dir, \"hi.translit.sampled.test.tsv\")\n\n    # build vocab on training set if requested\n    train_ds = TranslitDataset(train_file, build_vocab=build_vocab)\n    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n\n    # reuse vocabs for dev & test\n    val_ds  = TranslitDataset(val_file,  src_vocab, tgt_vocab)\n    test_ds = TranslitDataset(test_file, src_vocab, tgt_vocab)\n\n    # DataLoader instances\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=pad_collate)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n    test_loader  = DataLoader(test_ds,  batch_size=1,            shuffle=False, collate_fn=pad_collate)\n\n    return (\n        train_loader,\n        val_loader,\n        test_loader,\n        len(src_vocab),\n        len(tgt_vocab),\n        src_vocab[\"<pad>\"],\n        src_vocab,\n        tgt_vocab,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:18.655744Z","iopub.execute_input":"2025-05-20T19:17:18.655965Z","iopub.status.idle":"2025-05-20T19:17:18.671322Z","shell.execute_reply.started":"2025-05-20T19:17:18.655947Z","shell.execute_reply":"2025-05-20T19:17:18.670074Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class TrainingEarlyStopper:\n    \"\"\"\n    Halts training when the monitored metric stops improving.\n    \n    Args:\n        patience: number of checks with no improvement before stopping\n        min_delta: minimal increase to qualify as an improvement\n    \"\"\"\n    def __init__(self, patience: int = 5, min_delta: float = 1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_score = None\n        self.wait = 0\n\n    def should_stop(self, current_score: float) -> bool:\n        \"\"\"\n        Check if training should stop based on current metric.\n        \n        Args:\n            current_score: latest value of the monitored metric\n        Returns:\n            True if no improvement for `patience` calls, else False\n        \"\"\"\n        # first call or significant improvement\n        if self.best_score is None or current_score > self.best_score + self.min_delta:\n            self.best_score = current_score\n            self.wait = 0\n        else:\n            self.wait += 1\n        \n        # stop if waited too long\n        return self.wait >= self.patience\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:18.960113Z","iopub.execute_input":"2025-05-20T19:17:18.960331Z","iopub.status.idle":"2025-05-20T19:17:18.968364Z","shell.execute_reply.started":"2025-05-20T19:17:18.960312Z","shell.execute_reply":"2025-05-20T19:17:18.966795Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# ─── WandB Sweep Configuration ────────────────────────────────────────────────\nsweep_cfg = {\n    \"method\": \"bayes\",  # Bayesian hyperparameter search\n    \"metric\": {\n        \"name\": \"val_acc\", \n        \"goal\": \"maximize\"\n    },\n    \"early_terminate\": {\n        \"type\": \"hyperband\",\n        \"min_iter\": 2,\n        \"max_iter\": 8,\n        \"s\": 2\n    },\n    \"parameters\": {\n        # Model & training sizes\n        \"batch_size\":            {\"values\": [16, 32, 64, 128, 256]},\n        \"num_epochs\":            {\"values\": [10]},\n        \"encoder_layers\":        {\"values\": [1, 2, 3]},\n        \"decoder_layers\":        {\"values\": [1, 2, 3]},\n        \"hidden_size\":           {\"values\": [16, 32, 64, 128, 256, 512, 1024]},\n        \"embedding_dim\":         {\"values\": [16, 32, 64, 256, 512]},\n        \"dropout_rate\":          {\"values\": [0.2, 0.3, 0.4]},\n        \"bi_directional\":        {\"values\": [True, False]},\n        # Search & decoding\n        \"beam_width\":            {\"values\": [1, 3, 5]},\n        \"teacher_forcing_ratio\": {\"values\": [0.0, 0.3, 0.5, 0.7, 1.0]},\n        \"length_penalty\":        {\"values\": [0, 0.4, 0.5, 0.6]},\n        # Optimization\n        \"optimizer\":             {\"values\": [\"adam\", \"sgd\", \"rmsprop\", \"adagrad\"]},\n        \"learning_rate\":         {\"values\": [0.005, 0.001, 0.01, 0.1]},\n        # RNN cell variants\n        \"rnn_cell\":              {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:19.123934Z","iopub.execute_input":"2025-05-20T19:17:19.124186Z","iopub.status.idle":"2025-05-20T19:17:19.134075Z","shell.execute_reply.started":"2025-05-20T19:17:19.124166Z","shell.execute_reply":"2025-05-20T19:17:19.133325Z"}},"outputs":[{"name":"stdout","text":"\nFinal Test Acc: 0.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▄▅▄▇█▅</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▃▃█▁▁█▃▃▆</td></tr><tr><td>train_loss</td><td>▁▄▁▂▁▁█▁▁▅</td></tr><tr><td>val_acc</td><td>▁▁▁█▁▁█▁▁▁</td></tr><tr><td>val_loss</td><td>▁▄▁▂▁▁▅▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>test_accuracy</td><td>0</td></tr><tr><td>train_acc</td><td>0.00452</td></tr><tr><td>train_loss</td><td>42183814.66699</td></tr><tr><td>val_acc</td><td>0</td></tr><tr><td>val_loss</td><td>47228.40837</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rnn_dp20_bw1_tf70_emb32_hid128_enc1_dec2</strong> at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/runs/n55epajr' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/runs/n55epajr</a><br> View project at: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_191050-n55epajr/logs</code>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# ─── Optimized Sweep Parameters ───────────────────────────────────────────────\nbest_cfg = {\n    \"parameters\": {\n        # batch & epochs\n        \"batch_size\":            {\"values\": [64]},\n        \"num_epochs\":            {\"values\": [10]},\n        # model architecture\n        \"encoder_layers\":        {\"values\": [2]},\n        \"decoder_layers\":        {\"values\": [2]},\n        \"hidden_size\":           {\"values\": [256]},\n        \"embedding_dim\":         {\"values\": [64]},\n        \"dropout_rate\":          {\"values\": [0.4]},\n        \"bi_directional\":        {\"values\": [False]},\n        # decoding & loss\n        \"beam_width\":            {\"values\": [5]},\n        \"length_penalty\":        {\"values\": [0, 4]},\n        \"teacher_forcing_ratio\": {\"values\": [1.0]},\n        # optimization\n        \"optimizer\":             {\"values\": [\"adam\"]},\n        \"learning_rate\":         {\"values\": [0.001]},\n        # RNN variant\n        \"rnn_cell\":              {\"values\": [\"LSTM\"]},\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:29.902617Z","iopub.execute_input":"2025-05-20T19:17:29.903410Z","iopub.status.idle":"2025-05-20T19:17:29.909964Z","shell.execute_reply.started":"2025-05-20T19:17:29.903377Z","shell.execute_reply":"2025-05-20T19:17:29.908887Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def run_training():\n    # ─── Initialize W&B & Naming ──────────────────────────────────────────────\n    with wandb.init():\n        cfg = wandb.config\n        run_name = (\n            f\"{cfg.rnn_cell.lower()}_dp{int(cfg.dropout_rate*100)}\"\n            f\"_bw{cfg.beam_width}_tf{int(cfg.teacher_forcing_ratio*100)}\"\n            f\"_emb{cfg.embedding_dim}_hid{cfg.hidden_size}\"\n            f\"_enc{cfg.encoder_layers}_dec{cfg.decoder_layers}\"\n        )\n        wandb.run.name = run_name\n\n        # ─── Load Data & Build Vocab ────────────────────────────────────────────\n        train_loader, val_loader, test_loader, src_sz, tgt_sz, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n            LEXICON_ROOT, batch_size=cfg.batch_size, build_vocab=True\n        )\n        idx2char = {i: ch for ch, i in tgt_vocab.items()}\n\n        # ─── Model, Loss & Optimizer ────────────────────────────────────────────\n        model = Seq2SeqModel(\n            src_vocab=src_sz,\n            tgt_vocab=tgt_sz,\n            emb_dim=cfg.embedding_dim,\n            hid_dim=cfg.hidden_size,\n            enc_layers=cfg.encoder_layers,\n            dec_layers=cfg.decoder_layers,\n            rnn_type=cfg.rnn_cell,\n            drop_p=cfg.dropout_rate,\n            bidir_enc=cfg.bi_directional,\n        ).to(DEVICE)\n\n        # Use Adam optimizer exclusively\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n        criterion = nn.NLLLoss(ignore_index=pad_idx)\n        stopper = TrainingEarlyStopper(patience=5, min_delta=1e-4)\n        best_val_acc = 0.0\n\n        # ─── Training Loop ────────────────────────────────────────────────────────\n        for epoch in range(1, cfg.num_epochs + 1):\n            model.train()\n            total_loss = 0.0\n            correct, total = 0, 0\n\n            for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Train\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                optimizer.zero_grad()\n                outputs = model(src, tgt, tf_ratio=cfg.teacher_forcing_ratio)\n                loss = criterion(outputs.view(-1, tgt_sz), tgt.view(-1))\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n                # compute sequence accuracy\n                preds = outputs.argmax(dim=2)\n                for p, t in zip(preds, tgt):\n                    p_trim = p[1:][t[1:] != pad_idx]\n                    t_trim = t[1:][t[1:] != pad_idx]\n                    if torch.equal(p_trim, t_trim):\n                        correct += 1\n                    total += 1\n\n            train_acc = 100 * correct / total\n\n            # ─── Validation ────────────────────────────────────────────────────────\n            model.eval()\n            val_loss, correct, total = 0.0, 0, 0\n            with torch.no_grad():\n                for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Val\", leave=False):\n                    src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                    outputs = model(src, tgt, tf_ratio=0.0)\n                    val_loss += criterion(outputs.view(-1, tgt_sz), tgt.view(-1)).item()\n                    preds = outputs.argmax(dim=2)\n                    for p, t in zip(preds, tgt):\n                        p_trim = p[1:][t[1:] != pad_idx]\n                        t_trim = t[1:][t[1:] != pad_idx]\n                        if torch.equal(p_trim, t_trim):\n                            correct += 1\n                        total += 1\n\n            val_acc = 100 * correct / total\n            avg_val_loss = val_loss / len(val_loader)\n\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": total_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": avg_val_loss,\n                \"val_acc\": val_acc,\n            })\n            print(f\"[Epoch {epoch}] TL={total_loss:.3f} TA={train_acc:.2f}% │ VL={avg_val_loss:.3f} VA={val_acc:.2f}%\")\n\n            # early stopping\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n            elif stopper.should_stop(val_acc):\n                print(\"Early stopping triggered.\")\n                break\n\n        # ─── Final Test Evaluation ───────────────────────────────────────────────\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for src, tgt in tqdm(test_loader, desc=\"Test Eval\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                outputs = model(src, tgt, tf_ratio=0.0)\n                preds = outputs.argmax(dim=2)\n                for p, t in zip(preds, tgt):\n                    p_trim = p[1:][t[1:] != pad_idx]\n                    t_trim = t[1:][t[1:] != pad_idx]\n                    if torch.equal(p_trim, t_trim):\n                        correct += 1\n                    total += 1\n\n        test_acc = 100 * correct / total\n        print(f\"\\nFinal Test Acc: {test_acc:.2f}%\")\n        wandb.log({\"test_accuracy\": test_acc})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:17:33.594437Z","iopub.execute_input":"2025-05-20T19:17:33.595167Z","iopub.status.idle":"2025-05-20T19:17:33.609784Z","shell.execute_reply.started":"2025-05-20T19:17:33.595142Z","shell.execute_reply":"2025-05-20T19:17:33.608820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[Epoch 5] Val:   0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_35/10891828.py\", line 83, in run_training\n    wandb.log({\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\", line 36, in preinit_wrapper\n    raise wandb.Error(f\"You must call wandb.init() before {name}()\")\nwandb.errors.errors.Error: You must call wandb.init() before wandb.log()\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ─── Initiate Hyperparameter Sweep ─────────────────────────────────────────────\nsweep_id = wandb.sweep(sweep_cfg, project=\"cs24m020_dl_a3_v2\")\nwandb.agent(sweep_id, function=run_training, count=100)\n\n# ─── Optional Alternate Sweep Setup ────────────────────────────────────────────\n# alt_sweep = \"8espi10w\"\n# wandb.agent(\n#     sweep_id=alt_sweep,\n#     function=run_training,\n#     count=100,\n#     entity=\"cs24m020-indian-institute-of-technology-madras\",\n#     project=\"cs24m020_dl_a3_v2\",\n# )\n\n# ─── Finalize W&B Session ──────────────────────────────────────────────────────\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:43.506593Z","iopub.execute_input":"2025-05-20T19:10:43.507756Z","iopub.status.idle":"2025-05-20T19:10:44.808137Z","shell.execute_reply.started":"2025-05-20T19:10:43.507725Z","shell.execute_reply":"2025-05-20T19:10:44.807467Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: qv4thkjl\nSweep URL: https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/sweeps/qv4thkjl\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n55epajr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbi_directional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_cell: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.7\n\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_191050-n55epajr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/runs/n55epajr' target=\"_blank\">comic-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/sweeps/qv4thkjl' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/sweeps/qv4thkjl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/sweeps/qv4thkjl' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/sweeps/qv4thkjl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/runs/n55epajr' target=\"_blank\">https://wandb.ai/cs24m020-indian-institute-of-technology-madras/cs24m020_dl_a3_v2/runs/n55epajr</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[Epoch 1] Train:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Add this cell at the end of your notebook\n\n# First, get the data loaders and vocabs (same as in your training setup)\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\n# Define best configuration based on sweep results\nBEST_CONFIG = {\n    'embedding_dim': 64,\n    'hidden_size': 256,\n    'encoder_layers': 2,\n    'decoder_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout_p': 0.4,\n    'beam_width': 5,\n    'teacher_forcing_ratio': 1.0,\n    'bidirectional_encoder': False\n}\n\ndef evaluate_predictions(model, device, test_loader, tgt_vocab, beam_width=5):\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n    \n    input_words = []\n    decoded_outputs = []\n    correct_outputs = []\n    results = []\n    \n    # Create reverse vocabulary mapping\n    IDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}\n    IDX2CHAR_SRC = {idx: ch for ch, idx in src_vocab.items()}\n    \n    sos_idx = tgt_vocab['<sos>']\n    eos_idx = tgt_vocab['<eos>']\n    pad_idx = tgt_vocab['<pad>']\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(test_loader, desc=\"Evaluating\"):\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            # Get beam search results\n            beam_results = beam_search_decode(\n                model, src, sos_idx, eos_idx, \n                max_len=30, beam_width=beam_width, device=device\n            )\n            \n            # Get best prediction\n            best_seq = beam_results[0][0] if beam_results else []\n            \n            # Process target sequence (remove <sos>, <eos> and padding)\n            true_seq = []\n            for idx in tgt[0].tolist():\n                if idx == sos_idx:\n                    continue\n                if idx == eos_idx or idx == pad_idx:\n                    break\n                true_seq.append(idx)\n            \n            # Convert indices to characters\n            pred_chars = [IDX2CHAR_TGT.get(idx, '<unk>') for idx in best_seq[1:-1]]  # exclude <sos> and <eos>\n            true_chars = [IDX2CHAR_TGT.get(idx, '<unk>') for idx in true_seq]\n            \n            # Convert source indices to characters\n            src_chars = [IDX2CHAR_SRC.get(idx, '<unk>') for idx in src[0].tolist() if idx != pad_idx]\n            \n            # Check if prediction is correct\n            is_correct = pred_chars == true_chars\n            if is_correct:\n                total_correct += 1\n            total_samples += 1\n            \n            # Store results\n            input_words.append(''.join(src_chars))\n            decoded_outputs.append(''.join(pred_chars))\n            correct_outputs.append(''.join(true_chars))\n            results.append(\"Correct\" if is_correct else \"Incorrect\")\n    \n    # Calculate accuracy\n    accuracy = (total_correct / total_samples) * 100\n    print(f\"\\nEvaluation Results:\")\n    print(f\"Total Correct: {total_correct}\")\n    print(f\"Total Samples: {total_samples}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    \n    # Save results to CSV\n    results_df = pd.DataFrame({\n        'Input_Word': input_words,\n        'Decoded_Output': decoded_outputs,\n        'True_Output': correct_outputs,\n        'Match_Result': results\n    })\n    results_df.to_csv('predictions_beam_search.csv', index=False)\n    print(\"Predictions saved to 'predictions_beam_search.csv'\")\n    \n    return results_df\n\n# Initialize model with best configuration\nbest_model = Seq2Seq(\n    input_size=len(src_vocab),\n    output_size=len(tgt_vocab),\n    embedding_dim=BEST_CONFIG['embedding_dim'],\n    hidden_size=BEST_CONFIG['hidden_size'],\n    encoder_layers=BEST_CONFIG['encoder_layers'],\n    decoder_layers=BEST_CONFIG['decoder_layers'],\n    cell_type=BEST_CONFIG['cell_type'],\n    dropout_p=BEST_CONFIG['dropout_p'],\n    bidirectional_encoder=BEST_CONFIG['bidirectional_encoder']\n).to(DEVICE)\n\n# Note: You'll need to load your trained model weights here before evaluation\n# best_model.load_state_dict(torch.load('best_model_weights.pth'))\n\n# Run evaluation\nresults_df = evaluate_predictions(\n    best_model, \n    DEVICE, \n    test_loader, \n    tgt_vocab, \n    beam_width=BEST_CONFIG['beam_width']\n)\n\n# Display some sample predictions\nprint(\"\\nSample Predictions:\")\nsample_results = results_df.sample(min(5, len(results_df)))\nfor _, row in sample_results.iterrows():\n    print(f\"Input: {row['Input_Word']}\")\n    print(f\"True: {row['True_Output']}\")\n    print(f\"Pred: {row['Decoded_Output']}\")\n    print(f\"Result: {row['Match_Result']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:34.625168Z","iopub.execute_input":"2025-05-20T19:10:34.625733Z","iopub.status.idle":"2025-05-20T19:10:34.673149Z","shell.execute_reply.started":"2025-05-20T19:10:34.625705Z","shell.execute_reply":"2025-05-20T19:10:34.671907Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1232299864.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# First, get the data loaders and vocabs (same as in your training setup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BASE_DIR' is not defined"],"ename":"NameError","evalue":"name 'BASE_DIR' is not defined","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom tqdm import tqdm\nimport torch\n\n# Get data loaders and vocabs\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\n# Create reverse vocab mappings\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}\nIDX2CHAR_SRC = {idx: ch for ch, idx in src_vocab.items()}\nsos_idx = tgt_vocab['<sos>']\neos_idx = tgt_vocab['<eos>']\npad_idx = tgt_vocab['<pad>']\n\n# Best configuration\nBEST_CONFIG = {\n    'embedding_dim': 64,\n    'hidden_size': 256,\n    'encoder_layers': 2,\n    'decoder_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout_p': 0.4,\n    'beam_width': 5,\n    'teacher_forcing_ratio': 1.0,\n    'bidirectional_encoder': False\n}\n\ndef fast_beam_search(model, src, sos_idx, eos_idx, max_len=30, beam_width=5, device='cuda'):\n    \"\"\"Optimized single-instance beam search\"\"\"\n    model.eval()\n    with torch.no_grad():\n        encoder_outputs, encoder_hidden = model.encoder(src.unsqueeze(0))\n        \n        if model.bidirectional:\n            if model.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                c_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                for layer in range(model.decoder.num_layers):\n                    enc_layer = min(layer, model.encoder.num_layers - 1)\n                    h_combined = torch.cat((h_n[2*enc_layer], h_n[2*enc_layer+1]), dim=1)\n                    c_combined = torch.cat((c_n[2*enc_layer], c_n[2*enc_layer+1]), dim=1)\n                    h_dec[layer] = model.hidden_transform(h_combined)\n                    c_dec[layer] = model.hidden_transform(c_combined)\n                decoder_hidden = (h_dec, c_dec)\n            else:\n                decoder_hidden = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                for layer in range(model.decoder.num_layers):\n                    enc_layer = min(layer, model.encoder.num_layers - 1)\n                    h_combined = torch.cat((encoder_hidden[2*enc_layer], encoder_hidden[2*enc_layer+1]), dim=1)\n                    decoder_hidden[layer] = model.hidden_transform(h_combined)\n        else:\n            decoder_hidden = encoder_hidden\n\n        beams = [([sos_idx], 0.0, decoder_hidden)]\n        completed = []\n\n        for _ in range(max_len):\n            new_beams = []\n            for seq, score, hidden in beams:\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n                \n                input_char = torch.tensor([[seq[-1]]], device=device)\n                output, hidden_new = model.decoder(input_char, hidden)\n                log_probs = output.squeeze(0)\n                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n                \n                for k in range(beam_width):\n                    new_seq = seq + [topk_indices[k].item()]\n                    new_score = score + topk_log_probs[k].item()\n                    new_beams.append((new_seq, new_score, hidden_new))\n            \n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        completed += [(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx]\n        if not completed:\n            completed = beams\n            \n        return sorted(completed, key=lambda x: x[1], reverse=True)[0][0]\n\ndef generate_predictions_csv(model, test_loader, output_file='predictions.csv'):\n    \"\"\"Generate predictions and save to CSV with progress tracking\"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    results = {\n        'input': [],\n        'prediction': [],\n        'target': [],\n        'is_correct': [],\n        'input_length': [],\n        'prediction_length': []\n    }\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(test_loader, desc=\"Generating Predictions\"):\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            for i in range(src.size(0)):\n                # Process source\n                src_seq = src[i].tolist()\n                src_str = ''.join([IDX2CHAR_SRC.get(idx, '<unk>') for idx in src_seq if idx != pad_idx])\n                \n                # Process target\n                tgt_seq = []\n                for idx in tgt[i].tolist():\n                    if idx == sos_idx:\n                        continue\n                    if idx == eos_idx or idx == pad_idx:\n                        break\n                    tgt_seq.append(idx)\n                tgt_str = ''.join([IDX2CHAR_TGT.get(idx, '<unk>') for idx in tgt_seq])\n                \n                # Get prediction\n                pred_seq = fast_beam_search(\n                    model, src[i], sos_idx, eos_idx,\n                    beam_width=BEST_CONFIG['beam_width'],\n                    device=device\n                )\n                pred_str = ''.join([IDX2CHAR_TGT.get(idx, '<unk>') for idx in pred_seq[1:-1]])  # remove <sos> and <eos>\n                \n                # Store results\n                results['input'].append(src_str)\n                results['prediction'].append(pred_str)\n                results['target'].append(tgt_str)\n                results['is_correct'].append(pred_str == tgt_str)\n                results['input_length'].append(len(src_str))\n                results['prediction_length'].append(len(pred_str))\n    \n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(output_file, index=False)\n    \n    # Calculate and print statistics\n    accuracy = df['is_correct'].mean() * 100\n    avg_input_len = df['input_length'].mean()\n    avg_pred_len = df['prediction_length'].mean()\n    \n    print(f\"\\nPrediction Generation Complete\")\n    print(f\"Saved to: {output_file}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"Avg Input Length: {avg_input_len:.1f} chars\")\n    print(f\"Avg Prediction Length: {avg_pred_len:.1f} chars\")\n    \n    return df\n\n# Initialize model\nbest_model = Seq2Seq(\n    input_size=len(src_vocab),\n    output_size=len(tgt_vocab),\n    embedding_dim=BEST_CONFIG['embedding_dim'],\n    hidden_size=BEST_CONFIG['hidden_size'],\n    encoder_layers=BEST_CONFIG['encoder_layers'],\n    decoder_layers=BEST_CONFIG['decoder_layers'],\n    cell_type=BEST_CONFIG['cell_type'],\n    dropout_p=BEST_CONFIG['dropout_p'],\n    bidirectional_encoder=BEST_CONFIG['bidirectional_encoder']\n).to(DEVICE)\n\n# ─── Load Data & Build Vocab ────────────────────────────────────────────\ntrain_loader, val_loader, test_loader, src_sz, tgt_sz, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n        LEXICON_ROOT, batch_size=cfg.batch_size, build_vocab=True\n)\nidx2char = {i: ch for ch, i in tgt_vocab.items()}\n# ─── Model, Loss & Optimizer ────────────────────────────────────────────\nmodel = Seq2SeqModel(\n        src_vocab=src_sz,\n        tgt_vocab=tgt_sz,\n        emb_dim=64,\n        hid_dim=256,\n        enc_layers=2,\n        dec_layers=2,\n        rnn_type='LSTM',\n        drop_p=0.4,\n        bidir_enc=True,\n        ).to(DEVICE)\n# Load your trained model here (uncomment and modify path)\n# best_model.load_state_dict(torch.load('best_model.pth'))\n\n# Generate predictions and save to CSV\npredictions_df = generate_predictions_csv(best_model, test_loader, '/kaggle/working/transliteration_predictions.csv')\n\n# Display sample predictions\nprint(\"\\nSample Predictions:\")\nsample_df = predictions_df.sample(min(5, len(predictions_df)))\nfor _, row in sample_df.iterrows():\n    print(f\"\\nInput: {row['input']}\")\n    print(f\"Target: {row['target']}\")\n    print(f\"Prediction: {row['prediction']}\")\n    print(f\"Correct: {'✓' if row['is_correct'] else '✗'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:34.673978Z","iopub.status.idle":"2025-05-20T19:10:34.674251Z","shell.execute_reply.started":"2025-05-20T19:10:34.674129Z","shell.execute_reply":"2025-05-20T19:10:34.674141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom tqdm.auto import tqdm\n\n# ─── Evaluation & Prediction Dump ───────────────────────────────────────────────\ndef evaluate_and_save(model, test_loader, idx2char, pad_idx, device, batch_size=1):\n    \"\"\"\n    1) Runs the model on the test set\n    2) Computes and prints test accuracy\n    3) Writes each predicted sequence to /kaggle/working/predictions_vanilla/\n    \n    Args:\n        batch_size: Should be 1 for this implementation as we process one sequence at a time\n    \"\"\"\n    out_dir = \"/kaggle/working/predictions_vanilla\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    model.eval()\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for i, (src, tgt) in enumerate(tqdm(test_loader, desc=\"Test Eval\", leave=False)):\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Validate batch size\n            if src.size(0) != 1:\n                raise ValueError(f\"This implementation expects batch_size=1, got {src.size(0)}\")\n                \n            # do a forward pass (no teacher forcing)\n            logp= model(src, tgt, tf_ratio=0.0)\n            preds = logp.argmax(dim=2)[0].tolist()  # [0] assumes batch_size=1\n\n            # strip <sos>\n            preds = preds[1:]\n            # stop at first <eos> or pad\n            if pad_idx in preds:\n                preds = preds[: preds.index(pad_idx)]\n            \n            # likewise for the true target\n            true = tgt[0].tolist()[1:]\n            if pad_idx in true:\n                true = true[: true.index(pad_idx)]\n\n            # check exact-match accuracy\n            if preds == true:\n                correct += 1\n            total += 1\n\n            # convert tokens → characters and save\n            pred_str = \"\".join(idx2char[idx] for idx in preds)\n            with open(f\"{out_dir}/pred_{i:04d}.txt\", \"w\", encoding=\"utf-8\") as f:\n                f.write(pred_str)\n\n    acc = 100.0 * correct / total if total else 0.0\n    print(f\"Test Accuracy: {acc:.2f}%\")\n    return acc\n\n\n# ─── Usage Example ──────────────────────────────────────────────────────────────\nif __name__ == \"__main__\":\n    \n    # Set device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Set data path\n    LEXICON_ROOT = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons\"  # replace with actual path\n    \n    # Prepare the data\n    train_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n        LEXICON_ROOT, batch_size=64, build_vocab=True  # Note: batch_size=1 for evaluation\n    )\n    idx2char = {i: ch for ch, i in tgt_vocab.items()}\n\n    # Initialize model\n    model = Seq2SeqModel(\n        src_vocab=len(src_vocab),\n        tgt_vocab=len(tgt_vocab),\n        emb_dim=64,\n        hid_dim=256,\n        enc_layers=2,\n        dec_layers=2,\n        rnn_type='LSTM',\n        drop_p=0.4,\n        bidir_enc=False\n    ).to(DEVICE)\n    \n    # Load trained weights (uncomment after training)\n    # model.load_state_dict(torch.load(\"best_model.pth\"))\n    \n    # Evaluate the model and save predictions\n    evaluate_and_save(model, test_loader, idx2char, pad_idx, DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:02:15.737913Z","iopub.execute_input":"2025-05-20T20:02:15.738645Z","iopub.status.idle":"2025-05-20T20:02:35.066860Z","shell.execute_reply.started":"2025-05-20T20:02:15.738622Z","shell.execute_reply":"2025-05-20T20:02:35.066139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Test Eval:   0%|          | 0/4502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Test Accuracy: 0.00%\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"","metadata":{}}]}