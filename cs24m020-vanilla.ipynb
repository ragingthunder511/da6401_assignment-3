{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11873479,"sourceType":"datasetVersion","datasetId":7461882}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Standard Library Imports ===\nimport os            # File system utilities\nimport re            # Regular expressions\nimport random        # Randomization tools\n\n# === Data Handling & Progress Tracking ===\nimport pandas as pd              # DataFrame manipulation\nfrom tqdm.auto import tqdm       # Smart progress bar\nimport wandb                    # Experiment tracking\n\n# === PyTorch Core Modules ===\nimport torch                    # Base tensor library\nimport torch.nn as nn           # Neural net layers\nimport torch.optim as optim     # Optimizers (SGD, Adam, etc.)\nimport torch.nn.functional as F # Functional layer ops\nfrom torch.utils.data import Dataset, DataLoader  # Custom dataset/dataloader\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Dataset Path Configuration ===\n# Modify language code to switch lexicon, e.g. 'hi' → 'mr'\nLEXICON_DIR = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons\"\n\n# === WANDB Authentication Setup ===\nWANDB_API_KEY = \"8f2f82255a6e5ea16321da3895ae6b00d50eb5b5\"\nos.environ.setdefault(\"WANDB_API_KEY\", WANDB_API_KEY)\ntry:\n    # Force fresh login if necessary\n    wandb.login(key=WANDB_API_KEY, relogin=True)\nexcept wandb.errors.UsageError:\n    # Skip if already logged in or invalid key\n    pass\n\n# === Seed & Device Configuration ===\nSEED = 42\nrandom.seed(SEED)            # Seed Python RNG\ntorch.manual_seed(SEED)      # Seed PyTorch RNG\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Character Embedding Layer ===\nclass CharEmbedder(nn.Module):\n    \"\"\"Converts char indices to embeddings.\"\"\"\n    def __init__(self, vocab_sz: int, emb_dim: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_sz, emb_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Input shape: [batch, seq_len]\n        # Output shape: [batch, seq_len, emb_dim]\n        return self.embedding(x)\n\n\n# === Sequence Encoder (RNN/GRU/LSTM) ===\nclass SeqEncoder(nn.Module):\n    \"\"\"\n    Encodes sequences into context vectors.\n    Parameters:\n      vocab_sz: input vocab size\n      hid_dim: hidden layer size\n      emb_dim: embedding size\n      layers: RNN layers count\n      rnn_kind: 'GRU', 'LSTM', or 'RNN'\n      dropout_p: dropout rate\n      bidirectional: flag for bidirectional RNN\n    \"\"\"\n    def __init__(\n        self,\n        vocab_sz: int,\n        hid_dim: int,\n        emb_dim: int,\n        layers: int = 1,\n        rnn_kind: str = \"GRU\",\n        dropout_p: float = 0.1,\n        bidirectional: bool = False,\n    ):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.directions = 2 if bidirectional else 1\n\n        self.embedding = nn.Embedding(vocab_sz, emb_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        # Apply dropout between layers only if >1 layer\n        rnn_dropout = dropout_p if layers > 1 else 0.0\n\n        rnn_map = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}\n        rnn_args = dict(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=layers,\n            dropout=rnn_dropout,\n            bidirectional=bidirectional,\n            batch_first=True,\n        )\n        if rnn_kind == \"RNN\":\n            rnn_args[\"nonlinearity\"] = \"tanh\"\n\n        self.rnn = rnn_map[rnn_kind](**rnn_args)\n\n    def forward(self, tokens: torch.Tensor):\n        # tokens: [batch, seq_len]\n        emb = self.dropout(self.embedding(tokens))  # [batch, seq_len, emb_dim]\n        outputs, hidden_state = self.rnn(emb)\n        return outputs, hidden_state  # all steps output + final hidden (and cell)\n\n\n# === Stepwise Decoder for Tokens ===\nclass SeqDecoder(nn.Module):\n    \"\"\"\n    Step-by-step token generator.\n    Parameters:\n      vocab_sz: target vocab size\n      hid_dim: RNN hidden units\n      emb_dim: embedding size\n      layers: number of RNN layers\n      rnn_kind: 'GRU', 'LSTM', or 'RNN'\n      dropout_p: dropout before/after RNN\n    \"\"\"\n    def __init__(\n        self,\n        vocab_sz: int,\n        hid_dim: int,\n        emb_dim: int,\n        layers: int = 1,\n        rnn_kind: str = \"GRU\",\n        dropout_p: float = 0.1,\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_sz, emb_dim)\n        self.input_dropout = nn.Dropout(dropout_p)\n        rnn_dropout = dropout_p if layers > 1 else 0.0\n\n        rnn_map = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}\n        rnn_args = dict(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=layers,\n            dropout=rnn_dropout,\n            batch_first=True,\n        )\n        if rnn_kind == \"RNN\":\n            rnn_args[\"nonlinearity\"] = \"tanh\"\n\n        self.rnn = rnn_map[rnn_kind](**rnn_args)\n        self.output_dropout = nn.Dropout(rnn_dropout)\n        self.output_layer = nn.Linear(hid_dim, vocab_sz)\n\n    def forward(self, current_token: torch.Tensor, prev_state):\n        \"\"\"\n        Inputs:\n          current_token: [batch, 1], current input token indices\n          prev_state: prior hidden (and cell) states\n        Outputs:\n          log_probs: [batch, vocab_sz], log-probabilities of next tokens\n          next_state: updated RNN states\n        \"\"\"\n        emb = self.embedding(current_token)  # [batch, 1, emb_dim]\n        emb = self.input_dropout(emb)\n        rnn_out, next_state = self.rnn(emb, prev_state)\n        dropped_out = self.output_dropout(rnn_out[:, 0, :])  # timestep squeezed\n        logits = self.output_layer(dropped_out)\n        return F.log_softmax(logits, dim=-1), next_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def beam_search_decode(\n    model,\n    src_seq: torch.Tensor,\n    start_token: int,\n    end_token: int,\n    max_steps: int = 30,\n    beam_width: int = 3,\n    device: torch.device = DEVICE,\n):\n    \"\"\"Perform beam search decoding with a seq2seq model.\n\n    Args:\n      model: seq2seq model (encoder + decoder)\n      src_seq: input tensor batch [batch, seq_len]\n      start_token: start-of-sequence token index\n      end_token: end-of-sequence token index\n      max_steps: max decode length\n      beam_width: number of beams to track\n      device: compute device\n\n    Returns:\n      List of (token sequence, score) sorted by descending score\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Encode input batch\n        encoder_outputs, encoder_hidden = model.encoder(src_seq.to(device))\n\n        # If encoder is bidirectional, merge forward/backward states\n        if model.bidirectional:\n            def merge_states(h, c=None):\n                layers = model.encoder.num_layers\n                h_merged = torch.zeros(layers, 1, model.decoder.hidden_size, device=device)\n                c_merged = None if c is None else torch.zeros_like(h_merged)\n                for layer in range(layers):\n                    f_h, b_h = h[2*layer], h[2*layer + 1]\n                    h_merged[layer] = model.hidden_transform(torch.cat((f_h, b_h), dim=1))\n                    if c is not None:\n                        f_c, b_c = c[2*layer], c[2*layer + 1]\n                        c_merged[layer] = model.hidden_transform(torch.cat((f_c, b_c), dim=1))\n                return (h_merged, c_merged) if c is not None else h_merged\n\n            # Merge hidden states for LSTM or GRU/RNN\n            if isinstance(encoder_hidden, tuple):  # LSTM: (h, c)\n                decoder_state = merge_states(*encoder_hidden)\n            else:  # GRU or vanilla RNN\n                decoder_state = merge_states(encoder_hidden)\n        else:\n            # Use encoder hidden as decoder initial state\n            decoder_state = encoder_hidden\n\n        # Start beam search with start token\n        beams = [([start_token], 0.0, decoder_state)]  # (sequence, log_prob, hidden_state)\n        finished = []\n\n        # Expand beams step-by-step\n        for _ in range(max_steps):\n            candidates = []\n            for seq, log_prob, hidden in beams:\n                # If end token generated, save completed sequence\n                if seq[-1] == end_token:\n                    finished.append((seq, log_prob))\n                    continue\n                # Decode next step probabilities\n                input_token = torch.tensor([[seq[-1]]], device=device)\n                log_probs, next_hidden = model.decoder(input_token, hidden)\n\n                # Get top-k candidates\n                top_probs, top_indices = torch.topk(log_probs.squeeze(0), beam_width)\n                for prob, idx in zip(top_probs, top_indices):\n                    new_seq = seq + [idx.item()]\n                    new_log_prob = log_prob + prob.item()\n                    candidates.append((new_seq, new_log_prob, next_hidden))\n\n            # Keep top beam_width beams only\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        # Add remaining beams ending with end token\n        for seq, log_prob, _ in beams:\n            if seq[-1] == end_token:\n                finished.append((seq, log_prob))\n\n        # If no sequences ended properly, use best available beams\n        if not finished:\n            finished = [(seq, log_prob) for seq, log_prob, _ in beams]\n\n        # Return sorted sequences by score (high to low)\n        return sorted(finished, key=lambda x: x[1], reverse=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Seq2Seq wrapper: combines encoder & decoder, handles bidirectionality and teacher forcing ───\nclass Seq2SeqModel(nn.Module):\n    \"\"\"\n    Seq2Seq model wrapping encoder & decoder.\n    Supports bidirectional encoder with hidden state projection.\n    Uses teacher forcing during training.\n    \"\"\"\n    def __init__(\n        self,\n        src_vocab: int,\n        tgt_vocab: int,\n        emb_dim: int = 256,\n        hid_dim: int = 256,\n        enc_layers: int = 1,\n        dec_layers: int = 1,\n        rnn_type: str = \"GRU\",\n        drop_p: float = 0.2,\n        bidir_enc: bool = False,\n    ):\n        super().__init__()\n        # Encoder setup\n        self.encoder = SeqEncoder(\n            vocab_size=src_vocab,\n            hid_dim=hid_dim,\n            emb_dim=emb_dim,\n            layers=enc_layers,\n            rnn_type=rnn_type,\n            drop=drop_p,\n            bidir=bidir_enc,\n        )\n        self.bidir = bidir_enc\n        # Projection for bidirectional hidden states\n        if bidir_enc:\n            self.hidden_proj = nn.Linear(hid_dim * 2, hid_dim)\n        # Decoder setup\n        self.decoder = SeqDecoder(\n            out_size=tgt_vocab,\n            hid_dim=hid_dim,\n            emb_dim=emb_dim,\n            layers=dec_layers,\n            rnn_type=rnn_type,\n            drop=drop_p,\n        )\n        self.rnn_type = rnn_type\n\n    # Adjust hidden states to decoder layer count (trim or pad)\n    def _match_layers(self, h: torch.Tensor, bsz: int):\n        dl = self.decoder.rnn.num_layers\n        if h.size(0) > dl:\n            return h[:dl]\n        if h.size(0) < dl:\n            pad = torch.zeros(dl - h.size(0), bsz, h.size(2), device=h.device)\n            return torch.cat([h, pad], dim=0)\n        return h\n\n    def forward(self, src: torch.Tensor, tgt: torch.Tensor, tf_ratio: float = 0.5):\n        bsz, tgt_len = src.size(0), tgt.size(1)\n        out_vocab = self.decoder.project.out_features if hasattr(self.decoder, 'project') else self.decoder.project.weight.size(0)\n        outputs = torch.zeros(bsz, tgt_len, out_vocab, device=src.device)\n\n        # Encode source\n        enc_outs, enc_state = self.encoder(src)\n\n        # Initialize decoder state from encoder\n        if self.bidir:\n            if self.rnn_type == \"LSTM\":\n                h_n, c_n = enc_state\n                h_dec = torch.zeros(self.decoder.rnn.num_layers, bsz, self.decoder.rnn.hidden_size, device=src.device)\n                c_dec = torch.zeros_like(h_dec)\n                for i in range(self.decoder.rnn.num_layers):\n                    layer = min(i, self.encoder.rnn.num_layers - 1)\n                    # Concatenate forward & backward states and project\n                    h_cat = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    c_cat = torch.cat((c_n[2*layer], c_n[2*layer+1]), dim=1)\n                    h_dec[i] = self.hidden_proj(h_cat)\n                    c_dec[i] = self.hidden_proj(c_cat)\n                dec_state = (h_dec, c_dec)\n            else:\n                h_n = enc_state\n                h_dec = torch.zeros(self.decoder.rnn.num_layers, bsz, self.decoder.rnn.hidden_size, device=src.device)\n                for i in range(self.decoder.rnn.num_layers):\n                    layer = min(i, self.encoder.rnn.num_layers - 1)\n                    h_cat = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    h_dec[i] = self.hidden_proj(h_cat)\n                dec_state = h_dec\n        else:\n            # Match encoder-decoder layer counts for unidirectional\n            if self.rnn_type == \"LSTM\":\n                h, c = enc_state\n                dec_state = (self._match_layers(h, bsz), self._match_layers(c, bsz))\n            else:\n                dec_state = self._match_layers(enc_state, bsz)\n\n        # Decode with teacher forcing\n        input_tok = tgt[:, 0].unsqueeze(1)  # start token\n        for t in range(1, tgt_len):\n            probs, dec_state = self.decoder(input_tok, dec_state)\n            outputs[:, t, :] = probs\n            # Use target token or predicted token based on tf_ratio\n            top1 = probs.argmax(1).unsqueeze(1)\n            input_tok = tgt[:, t].unsqueeze(1) if random.random() < tf_ratio else top1\n\n        return outputs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ─── Dataset for transliteration pairs ───────────────────────────────\nclass TransliterationDataset(Dataset):\n    \"\"\"\n    Loads transliteration pairs from a TSV file.\n    Builds or accepts existing vocabularies.\n    \"\"\"\n    def __init__(self, file_path, src_vocab=None, tgt_vocab=None, create_vocab=False):\n        self.data_pairs = []\n        # Read each line and extract source and target sequences\n        with open(file_path, encoding=\"utf-8\") as file:\n            for line in file:\n                cols = line.strip().split(\"\\t\")\n                if len(cols) >= 2:\n                    tgt, src = cols[0], cols[1]\n                    self.data_pairs.append((src, tgt))\n\n        if create_vocab:\n            # Initialize vocab with special tokens\n            self.src_vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n            self.tgt_vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n            for src_seq, tgt_seq in self.data_pairs:\n                for ch in src_seq:\n                    self.src_vocab.setdefault(ch, len(self.src_vocab))\n                for ch in tgt_seq:\n                    self.tgt_vocab.setdefault(ch, len(self.tgt_vocab))\n        else:\n            # Use provided vocabularies\n            assert src_vocab is not None and tgt_vocab is not None, \"Vocabularies must be provided if not creating.\"\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n\n    def __len__(self):\n        return len(self.data_pairs)\n\n    def __getitem__(self, idx):\n        src_seq, tgt_seq = self.data_pairs[idx]\n\n        # Map characters to indices with <unk> fallback for source\n        src_indices = [self.src_vocab.get(ch, self.src_vocab[\"<unk>\"]) for ch in src_seq]\n        # For target, add <sos> and <eos> tokens around mapped indices\n        tgt_indices = [self.tgt_vocab[\"<sos>\"]] + [self.tgt_vocab.get(ch, self.tgt_vocab[\"<unk>\"]) for ch in tgt_seq] + [self.tgt_vocab[\"<eos>\"]]\n\n        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)\n\n\n# ─── Collate function to pad batches ───────────────────────────────\ndef pad_batch(batch):\n    \"\"\"\n    Pads sequences in batch to the length of the longest sequence.\n    Returns padded source and target tensors.\n    \"\"\"\n    src_seqs, tgt_seqs = zip(*batch)\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_tgt_len = max(len(seq) for seq in tgt_seqs)\n\n    PAD = 0\n    padded_src = torch.full((len(batch), max_src_len), PAD, dtype=torch.long)\n    padded_tgt = torch.full((len(batch), max_tgt_len), PAD, dtype=torch.long)\n\n    for i, (src, tgt) in enumerate(zip(src_seqs, tgt_seqs)):\n        padded_src[i, :len(src)] = src\n        padded_tgt[i, :len(tgt)] = tgt\n\n    return padded_src, padded_tgt\n\n\n# ─── Function to create DataLoaders ───────────────────────────────\ndef create_dataloaders(data_dir, batch_sz, build_vocab=False):\n    \"\"\"\n    Generates DataLoaders for train, validation, and test sets,\n    along with vocab sizes, pad token, and vocab dictionaries.\n    \"\"\"\n    train_path = os.path.join(data_dir, \"hi.translit.sampled.train.tsv\")\n    valid_path = os.path.join(data_dir, \"hi.translit.sampled.dev.tsv\")\n    test_path  = os.path.join(data_dir, \"hi.translit.sampled.test.tsv\")\n\n    train_dataset = TransliterationDataset(train_path, create_vocab=build_vocab)\n    src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n\n    val_dataset = TransliterationDataset(valid_path, src_vocab=src_vocab, tgt_vocab=tgt_vocab)\n    test_dataset = TransliterationDataset(test_path, src_vocab=src_vocab, tgt_vocab=tgt_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_sz, shuffle=True, collate_fn=pad_batch)\n    val_loader = DataLoader(val_dataset, batch_size=batch_sz, shuffle=False, collate_fn=pad_batch)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=pad_batch)\n\n    return (\n        train_loader,\n        val_loader,\n        test_loader,\n        len(src_vocab),\n        len(tgt_vocab),\n        src_vocab[\"<pad>\"],\n        src_vocab,\n        tgt_vocab,\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"\n    Stops training if a monitored metric doesn't improve.\n    \n    Args:\n      patience: max checks without improvement before stopping\n      min_delta: minimum required improvement to reset patience\n    \"\"\"\n    def __init__(self, patience: int = 5, min_delta: float = 1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_score = None\n        self.counter = 0\n\n    def check(self, metric: float) -> bool:\n        \"\"\"\n        Determine if training should stop.\n        \n        Args:\n          metric: current value of monitored metric\n        Returns:\n          True if no improvement for patience steps, else False\n        \"\"\"\n        if self.best_score is None or metric > self.best_score + self.min_delta:\n            self.best_score = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n        return self.counter >= self.patience\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── WandB Sweep Configuration ────────────────────────────────────────────────\nsweep_cfg = {\n    \"method\": \"bayes\",  # Bayesian hyperparameter search\n    \"metric\": {\n        \"name\": \"val_acc\", \n        \"goal\": \"maximize\"\n    },\n    \"early_terminate\": {\n        \"type\": \"hyperband\",\n        \"min_iter\": 2,\n        \"max_iter\": 8,\n        \"s\": 2\n    },\n    \"parameters\": {\n        # Model & training sizes\n        \"batch_size\":            {\"values\": [16, 32, 64, 128, 256]},\n        \"num_epochs\":            {\"values\": [10]},\n        \"encoder_layers\":        {\"values\": [1, 2, 3]},\n        \"decoder_layers\":        {\"values\": [1, 2, 3]},\n        \"hidden_size\":           {\"values\": [16, 32, 64, 128, 256, 512, 1024]},\n        \"embedding_dim\":         {\"values\": [16, 32, 64, 256, 512]},\n        \"dropout_rate\":          {\"values\": [0.2, 0.3, 0.4]},\n        \"bi_directional\":        {\"values\": [True, False]},\n        # Search & decoding\n        \"beam_width\":            {\"values\": [1, 3, 5]},\n        \"teacher_forcing_ratio\": {\"values\": [0.0, 0.3, 0.5, 0.7, 1.0]},\n        \"length_penalty\":        {\"values\": [0, 0.4, 0.5, 0.6]},\n        # Optimization\n        \"optimizer\":             {\"values\": [\"adam\", \"sgd\", \"rmsprop\", \"adagrad\"]},\n        \"learning_rate\":         {\"values\": [0.005, 0.001, 0.01, 0.1]},\n        # RNN cell variants\n        \"rnn_cell\":              {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Optimized Sweep Parameters ───────────────────────────────────────────────\nbest_cfg = {\n    \"parameters\": {\n        # batch & epochs\n        \"batch_size\":            {\"values\": [64]},\n        \"num_epochs\":            {\"values\": [10]},\n        # model architecture\n        \"encoder_layers\":        {\"values\": [2]},\n        \"decoder_layers\":        {\"values\": [2]},\n        \"hidden_size\":           {\"values\": [256]},\n        \"embedding_dim\":         {\"values\": [64]},\n        \"dropout_rate\":          {\"values\": [0.4]},\n        \"bi_directional\":        {\"values\": [False]},\n        # decoding & loss\n        \"beam_width\":            {\"values\": [5]},\n        \"length_penalty\":        {\"values\": [0, 4]},\n        \"teacher_forcing_ratio\": {\"values\": [1.0]},\n        # optimization\n        \"optimizer\":             {\"values\": [\"adam\"]},\n        \"learning_rate\":         {\"values\": [0.001]},\n        # RNN variant\n        \"rnn_cell\":              {\"values\": [\"LSTM\"]},\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model():\n    \"\"\"\n    Train Seq2Seq with W&B tracking, early stopping, and evaluation.\n    \"\"\"\n    with wandb.init():\n        cfg = wandb.config\n\n        # Compose run name for clarity in W&B\n        run_name = (\n            f\"{cfg.rnn_cell.lower()}_dp{int(cfg.dropout_rate*100)}\"\n            f\"_bw{cfg.beam_width}_tf{int(cfg.teacher_forcing_ratio*100)}\"\n            f\"_emb{cfg.embedding_dim}_hid{cfg.hidden_size}\"\n            f\"_enc{cfg.encoder_layers}_dec{cfg.decoder_layers}\"\n        )\n        wandb.run.name = run_name\n\n        # Load datasets and vocabularies\n        train_dl, val_dl, test_dl, src_vocab_size, tgt_vocab_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n            LEXICON_ROOT, batch_size=cfg.batch_size, build_vocab=True\n        )\n        idx_to_char = {i: ch for ch, i in tgt_vocab.items()}\n\n        # Initialize model, loss fn, optimizer, early stopper\n        model = Seq2SeqModel(\n            src_vocab=src_vocab_size,\n            tgt_vocab=tgt_vocab_size,\n            emb_dim=cfg.embedding_dim,\n            hid_dim=cfg.hidden_size,\n            enc_layers=cfg.encoder_layers,\n            dec_layers=cfg.decoder_layers,\n            rnn_type=cfg.rnn_cell,\n            drop_p=cfg.dropout_rate,\n            bidir_enc=cfg.bi_directional,\n        ).to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n        loss_fn = nn.NLLLoss(ignore_index=pad_idx)\n        early_stop = EarlyStopping(patience=5, min_delta=1e-4)\n        best_val_accuracy = 0.0\n\n        for epoch in range(1, cfg.num_epochs + 1):\n            model.train()\n            total_train_loss = 0.0\n            correct_preds, total_preds = 0, 0\n\n            for src_batch, tgt_batch in tqdm(train_dl, desc=f\"[Epoch {epoch}] Training\", leave=False):\n                src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n\n                optimizer.zero_grad()\n                output = model(src_batch, tgt_batch, tf_ratio=cfg.teacher_forcing_ratio)\n                loss = loss_fn(output.view(-1, tgt_vocab_size), tgt_batch.view(-1))\n                loss.backward()\n                optimizer.step()\n\n                total_train_loss += loss.item()\n\n                # Accuracy calculation per sequence\n                preds = output.argmax(dim=2)\n                for pred_seq, tgt_seq in zip(preds, tgt_batch):\n                    pred_trim = pred_seq[1:][tgt_seq[1:] != pad_idx]\n                    tgt_trim = tgt_seq[1:][tgt_seq[1:] != pad_idx]\n                    if torch.equal(pred_trim, tgt_trim):\n                        correct_preds += 1\n                    total_preds += 1\n\n            train_accuracy = 100 * correct_preds / total_preds\n\n            # Validation phase\n            model.eval()\n            val_loss_sum, val_correct, val_total = 0.0, 0, 0\n            with torch.no_grad():\n                for src_batch, tgt_batch in tqdm(val_dl, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n                    src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n                    output = model(src_batch, tgt_batch, tf_ratio=0.0)\n                    val_loss_sum += loss_fn(output.view(-1, tgt_vocab_size), tgt_batch.view(-1)).item()\n\n                    preds = output.argmax(dim=2)\n                    for pred_seq, tgt_seq in zip(preds, tgt_batch):\n                        pred_trim = pred_seq[1:][tgt_seq[1:] != pad_idx]\n                        tgt_trim = tgt_seq[1:][tgt_seq[1:] != pad_idx]\n                        if torch.equal(pred_trim, tgt_trim):\n                            val_correct += 1\n                        val_total += 1\n\n            val_accuracy = 100 * val_correct / val_total\n            avg_val_loss = val_loss_sum / len(val_dl)\n\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": total_train_loss,\n                \"train_accuracy\": train_accuracy,\n                \"val_loss\": avg_val_loss,\n                \"val_accuracy\": val_accuracy,\n            })\n\n            print(f\"[Epoch {epoch}] Train Loss={total_train_loss:.3f} Train Acc={train_accuracy:.2f}% | Val Loss={avg_val_loss:.3f} Val Acc={val_accuracy:.2f}%\")\n\n            # Early stopping check\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n            elif early_stop.check(val_accuracy):\n                print(\"Early stopping triggered.\")\n                break\n\n        # Test evaluation\n        model.eval()\n        test_correct, test_total = 0, 0\n        with torch.no_grad():\n            for src_batch, tgt_batch in tqdm(test_dl, desc=\"Testing\", leave=False):\n                src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n                output = model(src_batch, tgt_batch, tf_ratio=0.0)\n                preds = output.argmax(dim=2)\n                for pred_seq, tgt_seq in zip(preds, tgt_batch):\n                    pred_trim = pred_seq[1:][tgt_seq[1:] != pad_idx]\n                    tgt_trim = tgt_seq[1:][tgt_seq[1:] != pad_idx]\n                    if torch.equal(pred_trim, tgt_trim):\n                        test_correct += 1\n                    test_total += 1\n\n        test_accuracy = 100 * test_correct / test_total\n        print(f\"\\nFinal Test Accuracy: {test_accuracy:.2f}%\")\n        wandb.log({\"test_accuracy\": test_accuracy})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Initiate Hyperparameter Sweep ─────────────────────────────────────────────\nsweep_id = wandb.sweep(sweep_cfg, project=\"cs24m020_dl_a3_v2\")\nwandb.agent(sweep_id, function=run_training, count=100)\n\n# ─── Optional Alternate Sweep Setup ────────────────────────────────────────────\n# alt_sweep = \"8espi10w\"\n# wandb.agent(\n#     sweep_id=alt_sweep,\n#     function=run_training,\n#     count=100,\n#     entity=\"cs24m020-indian-institute-of-technology-madras\",\n#     project=\"cs24m020_dl_a3_v2\",\n# )\n\n# ─── Finalize W&B Session ──────────────────────────────────────────────────────\nwandb.finish()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get data loaders and vocabs\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\n# Reverse vocab mappings for easy idx-to-char conversion\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}\nIDX2CHAR_SRC = {idx: ch for ch, idx in src_vocab.items()}\n\nsos_idx = tgt_vocab['<sos>']\neos_idx = tgt_vocab['<eos>']\npad_idx = tgt_vocab['<pad>']\n\n# Best config from tuning/sweep results\nBEST_CONFIG = {\n    'embedding_dim': 64,\n    'hidden_size': 256,\n    'encoder_layers': 2,\n    'decoder_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout_p': 0.4,\n    'beam_width': 5,\n    'teacher_forcing_ratio': 1.0,\n    'bidirectional_encoder': False\n}\n\ndef fast_beam_search(model, src, sos_idx, eos_idx, max_len=30, beam_width=5, device='cuda'):\n    \"\"\"Optimized beam search for a single input sequence.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Encode the input\n        encoder_outputs, encoder_hidden = model.encoder(src.unsqueeze(0))\n        \n        # Prepare decoder hidden state if encoder is bidirectional\n        if model.bidirectional_encoder:\n            if model.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(model.decoder_layers, 1, model.hidden_size).to(device)\n                c_dec = torch.zeros(model.decoder_layers, 1, model.hidden_size).to(device)\n                for layer in range(model.decoder_layers):\n                    enc_layer = min(layer, model.encoder_layers - 1)\n                    h_combined = torch.cat((h_n[2*enc_layer], h_n[2*enc_layer+1]), dim=1)\n                    c_combined = torch.cat((c_n[2*enc_layer], c_n[2*enc_layer+1]), dim=1)\n                    h_dec[layer] = model.hidden_transform(h_combined)\n                    c_dec[layer] = model.hidden_transform(c_combined)\n                decoder_hidden = (h_dec, c_dec)\n            else:\n                decoder_hidden = torch.zeros(model.decoder_layers, 1, model.hidden_size).to(device)\n                for layer in range(model.decoder_layers):\n                    enc_layer = min(layer, model.encoder_layers - 1)\n                    h_combined = torch.cat((encoder_hidden[2*enc_layer], encoder_hidden[2*enc_layer+1]), dim=1)\n                    decoder_hidden[layer] = model.hidden_transform(h_combined)\n        else:\n            decoder_hidden = encoder_hidden\n\n        beams = [([sos_idx], 0.0, decoder_hidden)]\n        completed = []\n\n        for _ in range(max_len):\n            new_beams = []\n            for seq, score, hidden in beams:\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n                \n                input_char = torch.tensor([[seq[-1]]], device=device)\n                output, hidden_new = model.decoder(input_char, hidden)\n                log_probs = output.squeeze(0)\n                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n                \n                for k in range(beam_width):\n                    new_seq = seq + [topk_indices[k].item()]\n                    new_score = score + topk_log_probs[k].item()\n                    new_beams.append((new_seq, new_score, hidden_new))\n            \n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        completed += [(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx]\n        if not completed:\n            completed = beams\n            \n        return sorted(completed, key=lambda x: x[1], reverse=True)[0][0]\n\n\ndef generate_predictions_csv(model, test_loader, output_file='predictions.csv'):\n    \"\"\"Generate predictions for the test set and save results to a CSV file.\"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    results = {\n        'input': [],\n        'prediction': [],\n        'target': [],\n        'is_correct': [],\n        'input_length': [],\n        'prediction_length': []\n    }\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(test_loader, desc=\"Generating Predictions\"):\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            for i in range(src.size(0)):\n                # Convert source indices to string, ignoring padding\n                src_seq = src[i].tolist()\n                src_str = ''.join([IDX2CHAR_SRC.get(idx, '<unk>') for idx in src_seq if idx != pad_idx])\n                \n                # Process target sequence, strip <sos>, <eos>, and padding\n                tgt_seq = []\n                for idx in tgt[i].tolist():\n                    if idx == sos_idx:\n                        continue\n                    if idx == eos_idx or idx == pad_idx:\n                        break\n                    tgt_seq.append(idx)\n                tgt_str = ''.join([IDX2CHAR_TGT.get(idx, '<unk>') for idx in tgt_seq])\n                \n                # Run beam search prediction\n                pred_seq = fast_beam_search(\n                    model, src[i], sos_idx, eos_idx,\n                    beam_width=BEST_CONFIG['beam_width'],\n                    device=device\n                )\n                # Remove <sos> and <eos> tokens from prediction\n                pred_str = ''.join([IDX2CHAR_TGT.get(idx, '<unk>') for idx in pred_seq[1:-1]])\n                \n                # Record results\n                results['input'].append(src_str)\n                results['prediction'].append(pred_str)\n                results['target'].append(tgt_str)\n                results['is_correct'].append(pred_str == tgt_str)\n                results['input_length'].append(len(src_str))\n                results['prediction_length'].append(len(pred_str))\n    \n    # Save to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(output_file, index=False)\n    \n    # Print stats\n    accuracy = df['is_correct'].mean() * 100\n    avg_input_len = df['input_length'].mean()\n    avg_pred_len = df['prediction_length'].mean()\n    \n    print(f\"\\nPrediction Generation Complete\")\n    print(f\"Saved to: {output_file}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"Avg Input Length: {avg_input_len:.1f} chars\")\n    print(f\"Avg Prediction Length: {avg_pred_len:.1f} chars\")\n    \n    return df\n\n# Initialize model with best config\nbest_model = Seq2Seq(\n    input_size=len(src_vocab),\n    output_size=len(tgt_vocab),\n    embedding_dim=BEST_CONFIG['embedding_dim'],\n    hidden_size=BEST_CONFIG['hidden_size'],\n    encoder_layers=BEST_CONFIG['encoder_layers'],\n    decoder_layers=BEST_CONFIG['decoder_layers'],\n    cell_type=BEST_CONFIG['cell_type'],\n    dropout_p=BEST_CONFIG['dropout_p'],\n    bidirectional_encoder=BEST_CONFIG['bidirectional_encoder']\n).to(DEVICE)\n\n# ─── Load Data & Build Vocab ────────────────────────────────────────────\ntrain_loader, val_loader, test_loader, src_sz, tgt_sz, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n        LEXICON_ROOT, batch_size=cfg.batch_size, build_vocab=True\n)\nidx2char = {i: ch for ch, i in tgt_vocab.items()}\n\n# ─── Model, Loss & Optimizer ────────────────────────────────────────────\nmodel = Seq2SeqModel(\n        src_vocab=src_sz,\n        tgt_vocab=tgt_sz,\n        emb_dim=64,\n        hid_dim=256,\n        enc_layers=2,\n        dec_layers=2,\n        rnn_type='LSTM',\n        drop_p=0.4,\n        bidir_enc=True,\n        ).to(DEVICE)\n\n# Load trained weights here\n# best_model.load_state_dict(torch.load('best_model.pth'))\n\n# Generate predictions CSV\npredictions_df = generate_predictions_csv(best_model, test_loader, '/kaggle/working/predictions_vanilla/output.csv')\n\n# Show sample predictions\nprint(\"\\nSample Predictions:\")\nsample_df = predictions_df.sample(min(5, len(predictions_df)))\nfor _, row in sample_df.iterrows():\n    print(f\"\\nInput: {row['input']}\")\n    print(f\"Target: {row['target']}\")\n    print(f\"Prediction: {row['prediction']}\")\n    print(f\"Correct: {'✓' if row['is_correct'] else '✗'}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}